import os
import logging
from datasets import load_dataset
import yaml
import argparse
from typing import List
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from llamarec import LlamaRecForCausalLM, LlamaRecConfig
from sasrec import SasRecForCausalLM, SasRecConfig
from transformers import PreTrainedTokenizerFast, AutoTokenizer
from util.datacollator import EvalDataCollator
from util.utils_evaluate import build_item_token_codebooks_dynamically, beamsearch_prefix_constraint_fn
from util.eval import compute_hr_at_k, compute_ndcg_at_k
logging.basicConfig(level=logging.INFO)



def main():
    # è·å–é…ç½®æ–‡ä»¶è·¯å¾„
    parser = argparse.ArgumentParser(description="Train a LlamaRec model using a YAML config file.")
    parser.add_argument("--dataset", type=str, default='KuaiRand_27K_pt')
    parser.add_argument("--model_name", type=str, default='llamarec')
    parser.add_argument("--checkpoint", type=str, default='experiment/KuaiRand_27K_pt/llama-rec_20251212_013006/checkpoint-20000')
    args = parser.parse_args()

    # è¯»å–å¹¶è§£æ YAML é…ç½®æ–‡ä»¶
    logging.info(f"Loading configuration from: {args.dataset}_{args.model_name}")
    config_path = os.path.join("pretrain_config", args.dataset+'_'+ args.model_name + '.yaml')
    with open(config_path, 'r') as f:
        config_data = yaml.safe_load(f)

    # ä»è§£æçš„æ–‡ä»¶ä¸­æå–é…ç½®
    paths_config = config_data['paths']
    model_params = config_data['model_params']
    training_args_dict = config_data['training_args']
    tokenizer_params = config_data['tokenizer_params']
    testing_args = config_data['testing_args']

    # ä½¿ç”¨ä»é…ç½®ä¸­è¯»å–çš„å‚æ•°
    dataset_path = os.path.join(paths_config['dataset_path'], 'train.json')
    tokenizer_dir = paths_config['tokenizer_dir']
    max_seq_length = model_params['max_seq_length']
    generation_length = tokenizer_params['codebook_num']

    checkpoint_path = args.checkpoint
    output_dir = os.path.dirname(checkpoint_path)

    # ç›´æ¥ä»checkpointåŠ è½½tokenizer
    logging.info(f"Loading tokenizer from checkpoint: {checkpoint_path}")
    try:
        # tokenizer = PreTrainedTokenizerFast.from_pretrained(checkpoint_path)
        # ä½¿ç”¨ AutoTokenizerï¼Œå¹¶æ˜¾å¼æŒ‡å®š use_fast=Trueï¼ˆå¦‚æœéœ€è¦ï¼‰å’Œ trust_remote_code=Trueï¼ˆé˜²æ­¢è‡ªå®šä¹‰æ¨¡å‹æŠ¥é”™ï¼‰
        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, use_fast=True, trust_remote_code=True)
        logging.info("Tokenizer loaded successfully from checkpoint.")

    except Exception as e:
        logging.warning(f"Failed to load tokenizer from checkpoint: {e}")
        # å›é€€åˆ°ä»tokenizer_diråŠ è½½
        logging.info("Falling back to loading tokenizer from tokenizer_dir...")
        # tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_dir)
        try:
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, use_fast=True, trust_remote_code=True)
            logging.info("Tokenizer loaded successfully from tokenizer_dir.")
        except Exception as fallback_error:
            logging.error(f"Critical Error: Failed to load tokenizer from both locations. Error: {fallback_error}")
            raise fallback_error

    tokenizer.padding_side = 'left'

    # è¡¥ä¸ï¼šç¡®ä¿ pad_token å­˜åœ¨ (Qwen ç­‰æ¨¡å‹æœ‰æ—¶é»˜è®¤æ²¡æœ‰ pad_token)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        logging.info(f"Pad token was None, set to EOS token id: {tokenizer.pad_token_id}")

    # æ•°æ®é›†åŠ è½½
    test_dataset = load_dataset("json", data_files=dataset_path, split='train')

    # ç›´æ¥ä»checkpointåŠ è½½æ¨¡å‹
    logging.info(f"Loading model from checkpoint: {checkpoint_path}")
    
    # æ ¹æ® model_name å†³å®šä½¿ç”¨å“ªä¸ªç±»
    if args.model_name == 'sasrec':
        model_class = SasRecForCausalLM
        config_class = SasRecConfig
        # SasRec ä½¿ç”¨ layer_norm_eps
        norm_eps_key = 'layer_norm_eps'
        norm_eps_val = model_params.get('layer_norm_eps', 1e-12)
    else:
        model_class = LlamaRecForCausalLM
        config_class = LlamaRecConfig
        # LlamaRec ä½¿ç”¨ rms_norm_eps
        norm_eps_key = 'rms_norm_eps'
        norm_eps_val = model_params.get('rms_norm_eps', 1e-6)

    try:
        model = model_class.from_pretrained(checkpoint_path)
        logging.info(f"Model ({args.model_name}) loaded successfully from checkpoint")
    except Exception as e:
        logging.error(f"Failed to load model from checkpoint: {e}")
        # å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°åˆ›å»ºæ–°æ¨¡å‹ï¼ˆä½†æƒé‡ä¸åŒï¼‰
        logging.warning("Creating new model architecture (weights will be random!)")
        
        config_kwargs = {
            "hidden_size": model_params['hidden_size'],
            "intermediate_size": model_params['intermediate_size'],
            "num_hidden_layers": model_params['num_hidden_layers'],
            "num_attention_heads": model_params['num_attention_heads'],
            "max_position_embeddings": max_seq_length + generation_length,
            "model_type": model_params.get('MODEL_TYPE', args.model_name),
            "vocab_size": len(tokenizer),
            "use_cache": False,
            "pad_token_id": tokenizer.pad_token_id,
            "bos_token_id": tokenizer.bos_token_id,
            "eos_token_id": tokenizer.eos_token_id,
        }
        # æ³¨å…¥ç‰¹å®šçš„ Norm å‚æ•°
        config_kwargs[norm_eps_key] = norm_eps_val
        
        config = config_class(**config_kwargs)
        model = model_class(config)

    # try:
    #     model = LlamaRecForCausalLM.from_pretrained(checkpoint_path)
    #     logging.info(f"Model loaded successfully from checkpoint")
    # except Exception as e:
    #     logging.error(f"Failed to load model from checkpoint: {e}")
    #     # å¦‚æœå¤±è´¥ï¼Œå›é€€åˆ°åˆ›å»ºæ–°æ¨¡å‹ï¼ˆä½†æƒé‡ä¸åŒï¼‰
    #     logging.warning("Creating new model architecture (weights will be random!)")
    #     config = LlamaRecConfig(
    #         hidden_size=model_params['hidden_size'],
    #         intermediate_size=model_params['intermediate_size'],
    #         num_hidden_layers=model_params['num_hidden_layers'],
    #         num_attention_heads=model_params['num_attention_heads'],
    #         max_position_embeddings=max_seq_length + generation_length,
    #         rms_norm_eps=model_params['rms_norm_eps'],
    #         model_type=model_params['MODEL_TYPE'],
    #         vocab_size=len(tokenizer),
    #         use_cache=False,
    #         pad_token_id=tokenizer.pad_token_id,
    #         bos_token_id=tokenizer.bos_token_id,
    #         eos_token_id=tokenizer.eos_token_id,
    #     )
    #     model = LlamaRecForCausalLM(config)
    
    # å°†æ¨¡å‹ç§»åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # DataCollatorå®ä¾‹åŒ–
    test_collator = EvalDataCollator(tokenizer=tokenizer, max_length=max_seq_length)

    # è¯„ä¼°
    logging.info("Starting custom evaluation...")
    model.eval()
    
    # æ„å»ºè¯„ä¼° DataLoader
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=training_args_dict['per_device_eval_batch_size'],
        collate_fn=test_collator,
        shuffle=False,
    )

    # æ„é€  Item Token çº¦æŸç æœ¬
    item_token_codebooks = build_item_token_codebooks_dynamically(tokenizer, generation_length)

    k_values=testing_args['eval_k_values']
    num_beams=testing_args['num_beams']
    total_metrics_sum = {f"HR@{k}": 0.0 for k in k_values}
    total_metrics_sum.update({f"NDCG@{k}": 0.0 for k in k_values})
    total_samples = 0
    logging.info(f"Starting manual evaluation with num_beams={num_beams}...")

    with torch.no_grad():
        logging.info("Starting manual evaluation loop...")
        for batch in tqdm(test_dataloader, desc="Evaluating"):
            # 1. å‡†å¤‡è¾“å…¥
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            groundtruth = batch['groundtruth']

            prompt_length = input_ids.shape[1] 
            # å®šä¹‰beamsearchçº¦æŸçš„é—­åŒ…å‡½æ•°
            def batch_beamsearch_prefix_constraint_fn(batch_id: int, input_ids_tensor: torch.Tensor) -> List[int]:
                # è°ƒç”¨ä¸Šé¢å®šä¹‰çš„beamsearchçº¦æŸå‡½æ•°ï¼Œå¹¶ä¼ å…¥æ‰€æœ‰æ•è·çš„å‚æ•°
                # æ³¨æ„ï¼šbatch_id åœ¨è¿™é‡Œé€šå¸¸è¢«å¿½ç•¥ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯å¯¹æ‰€æœ‰æ ·æœ¬åº”ç”¨ç›¸åŒçš„çº¦æŸ
                return beamsearch_prefix_constraint_fn(
                    batch_id=batch_id,
                    input_ids_tensor=input_ids_tensor,
                    prompt_length=prompt_length,
                    generation_length=generation_length,
                    item_token_codebooks=item_token_codebooks # æ•è·çš„çº¦æŸåˆ—è¡¨
                )
            # 2. é¢„æµ‹ä¸‹ä¸€ä¸ª item çš„ tokens, ä½¿ç”¨è‡ªå®šä¹‰çš„ beam search çº¦æŸæ¥ç¡®ä¿åªç”Ÿæˆæœ‰æ•ˆçš„ item tokens
            generated_ids = model.generate( # (batch_size * num_beams, total_length)
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_length=input_ids.shape[1] + generation_length, 
                num_beams=num_beams,
                do_sample=False, # é‡‡æ ·è§£ç 
                num_return_sequences=num_beams, # è¿”å› num_beams ä¸ªåºåˆ—
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                prefix_allowed_tokens_fn=batch_beamsearch_prefix_constraint_fn
            )
            
            # 3. è§£ç å¹¶æå–æ¨èåˆ—è¡¨
            # æå–æ–°ç”Ÿæˆçš„ tokens (æœ€å generation_length ä¸ª tokens)
            new_tokens = generated_ids[:, -generation_length:] # (batch_size * num_beams, generation_length)
            predicted_token_sequences = tokenizer.batch_decode(new_tokens, skip_special_tokens=False)
            reshaped_token_sequences = [
                predicted_token_sequences[i : i + num_beams]
                for i in range(0, len(predicted_token_sequences), num_beams)
            ]
            # è°ƒç”¨è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°
            current_batch_size = len(reshaped_token_sequences)
            batch_hr = compute_hr_at_k(reshaped_token_sequences, groundtruth, k_values)
            batch_ndcg = compute_ndcg_at_k(reshaped_token_sequences, groundtruth, k_values)
            
            for k_val in k_values:
                total_metrics_sum[f"HR@{k_val}"] += batch_hr[f"HR@{k_val}"] * current_batch_size
                total_metrics_sum[f"NDCG@{k_val}"] += batch_ndcg[f"NDCG@{k_val}"] * current_batch_size
            
            total_samples += current_batch_size
            
    metrics = {name: (val / total_samples) for name, val in total_metrics_sum.items()}
    # è®°å½•å‚æ•°å’ŒæŒ‡æ ‡
    log_file = os.path.join(output_dir, "evaluate_log.txt")
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write("\n" + "=" * 50 + "\n")
        f.write(f"{checkpoint_path} metrics:\n")
        for metric_name, value in metrics.items():
            f.write(f"{metric_name}: {value}\n")
        f.write(f"testing_args:\n")
        for key, value in tokenizer_params.items():
            f.write(f"  {key}: {value}\n")
        for key, value in testing_args.items():
            f.write(f"  {key}: {value}\n")
    logging.info(f"Evaluation results: {metrics}")
    logging.info(f"ğŸ“ Results saved to: {log_file}")

    logging.info("All operations complete!")

if __name__ == "__main__":
    main()