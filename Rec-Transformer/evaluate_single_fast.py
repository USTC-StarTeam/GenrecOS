import os
import json
import logging
import yaml
import argparse
import sys
import torch
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import (
    AutoConfig, 
    AutoModelForCausalLM,
    LogitsProcessorList,
    AutoTokenizer,
    LlamaForCausalLM, LlamaConfig,
    Qwen2ForCausalLM, Qwen2Config,
)

# === å¯¼å…¥é¡¹ç›®ä¾èµ– (ç¡®ä¿è·¯å¾„æ­£ç¡®) ===
sys.path.append("../")
from llamarec import LlamaRecForCausalLM, LlamaRecConfig
from sasrec import SasRecForCausalLM, SasRecConfig
from utils.datacollator import EvalDataCollator, preprocess_function
from utils.utils_evaluate import (
    build_item_token_codebooks_dynamically, 
    DynamicHierarchicalLogitsProcessor,
)
from utils.eval import compute_hr_at_k, compute_ndcg_at_k
from utils.tokenizer_utils import create_pure_id_qwen_tokenizer

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

class StandaloneEvaluator:
    def __init__(self, model, tokenizer, generation_config_params, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.gen_len = generation_config_params['generation_length']
        self.num_beams = generation_config_params['num_beams']
        self.k_values = generation_config_params['k_values']
        self.item_token_codebooks = generation_config_params['item_token_codebooks']

        # --- å¤åˆ» CustomTrainer çš„ NumPy å‘é‡åŒ–æŸ¥æ‰¾è¡¨é€»è¾‘ ---
        logger.info(">>> Building NumPy Vectorized Vocab Lookup Table for fast eval...")
        vocab = tokenizer.get_vocab()
        max_id = max(vocab.values())
        self.vocab_array = np.array(["" for _ in range(max_id + 1)], dtype=object)
        for k, v in vocab.items():
            self.vocab_array[v] = k
        logger.info("âœ… Vocab Table built.")

    def evaluate(self, eval_dataloader):
        self.model.eval()
        
        total_metrics_sum = {f"HR@{k}": 0.0 for k in self.k_values}
        total_metrics_sum.update({f"NDCG@{k}": 0.0 for k in self.k_values})
        total_samples = 0

        logger.info(f"***** Running Evaluation *****")
        logger.info(f"  Num examples = {len(eval_dataloader.dataset)}")
        logger.info(f"  Batch size = {eval_dataloader.batch_size}")

        with torch.no_grad():
            for batch in tqdm(eval_dataloader, desc="Evaluating"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                groundtruth = batch['groundtruth']

                batch_size = input_ids.shape[0]
                prompt_length = input_ids.shape[1]

                # 1. æ„å»º Logits Processor
                logits_processor = LogitsProcessorList([
                    DynamicHierarchicalLogitsProcessor(
                        prompt_length=prompt_length,
                        item_token_codebooks=self.item_token_codebooks,
                        device=self.device
                    )
                ])

                # 2. ç”Ÿæˆ (Generate)
                generated_ids = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=prompt_length + self.gen_len,
                    num_beams=self.num_beams,
                    do_sample=False, # Eval æ—¶é€šå¸¸å›ºå®šä¸º False
                    num_return_sequences=self.num_beams,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    logits_processor=logits_processor, 
                    use_cache=True
                )

                # 3. å‘é‡åŒ–è§£ç  (Vectorized Decoding)
                new_tokens_cpu = generated_ids[:, -self.gen_len:].cpu().numpy()
                token_strs = self.vocab_array[new_tokens_cpu] # O(1) æŸ¥è¡¨

                # å­—ç¬¦ä¸²æ‹¼æ¥
                if self.gen_len == 1:
                    predicted_token_sequences = token_strs.flatten().tolist()
                else:
                    # ä½¿ç”¨ numpy çš„å­—ç¬¦ä¸²æ‹¼æ¥èƒ½åŠ›
                    result_array = token_strs[:, 0]
                    for i in range(1, self.gen_len):
                        result_array = result_array + token_strs[:, i]
                    predicted_token_sequences = result_array.tolist()

                # Reshape ä¸º [Batch, Beam]
                reshaped_token_sequences = [
                    predicted_token_sequences[i : i + self.num_beams]
                    for i in range(0, len(predicted_token_sequences), self.num_beams)
                ]

                # 4. è®¡ç®—æŒ‡æ ‡ (Metrics)
                batch_hr = compute_hr_at_k(reshaped_token_sequences, groundtruth, self.k_values)
                batch_ndcg = compute_ndcg_at_k(reshaped_token_sequences, groundtruth, self.k_values)

                for k_val in self.k_values:
                    total_metrics_sum[f"HR@{k_val}"] += batch_hr[f"HR@{k_val}"] * batch_size
                    total_metrics_sum[f"NDCG@{k_val}"] += batch_ndcg[f"NDCG@{k_val}"] * batch_size
                
                total_samples += batch_size

        # æ±‡æ€»
        if total_samples == 0:
            return {k: 0.0 for k in total_metrics_sum.keys()}
        
        metrics = {k: (v / total_samples) for k, v in total_metrics_sum.items()}
        return metrics

def main():
    parser = argparse.ArgumentParser(description="Standalone Evaluation for LlamaRec/SasRec")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="Path to the checkpoint folder (containing config.json and model.safetensors)")
    parser.add_argument("--dataset", type=str, default='Beauty', help="Dataset name used in config path")
    parser.add_argument("--model_name", type=str, default='llamarec', help="Model name used in config path")
    parser.add_argument("--split", type=str, default='test', choices=['train', 'test', 'valid'], help="Which split to evaluate")
    parser.add_argument("--batch_size", type=int, default=None, help="Override batch size from config")
    parser.add_argument("--output_file", type=str, default=None, help="Where to save metrics json")
    args = parser.parse_args()

    # 1. ç¡®å®šè®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"Using device: {device}")

    # 2. è¯»å–åŸå§‹ Config (ç”¨äºè·å–æ•°æ®è·¯å¾„ã€Tokené…ç½®ç­‰)
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(current_script_dir, "pretrain_config", args.dataset, f"{args.model_name}.yaml")
    
    logger.info(f"Loading original training config from: {config_path}")
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config not found at {config_path}")

    with open(config_path, 'r') as f:
        config_data = yaml.safe_load(f)

    paths_config = config_data['paths']
    tokenizer_params = config_data['tokenizer_params']
    training_args = config_data['training_args']
    testing_args = config_data['testing_args']
    model_params = config_data['model_params']

    dataset_path = paths_config['dataset_path']
    tokenizer_dir = paths_config['tokenizer_dir']
    max_seq_length = model_params['max_seq_length']
    
    # è·å– generation length
    codeword_nums = tokenizer_params.get('codeword_nums', [20, 20, 20])
    generation_length = len(codeword_nums)

# 3. åŠ è½½ Tokenizer
    logger.info(f"Loading Tokenizer from checkpoint: {args.checkpoint_path}")
    try:
        # å°è¯•ç›´æ¥ä» Checkpoint æ–‡ä»¶å¤¹åŠ è½½
        # trust_remote_code=True æ˜¯ä¸ºäº†é˜²æ­¢å¦‚æœæ˜¯è‡ªå®šä¹‰ Tokenizer ä»£ç 
        tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path, trust_remote_code=True)
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to load tokenizer directly from checkpoint: {e}")
        logger.warning("ğŸ”„ Falling back to rebuilding tokenizer from config...")
        
        # å¦‚æœåŠ è½½å¤±è´¥ï¼ˆæ¯”å¦‚æ–‡ä»¶ç¼ºå¤±ï¼‰ï¼Œåˆ™å›é€€åˆ°é‡å»ºé€»è¾‘
        tokenizer = create_pure_id_qwen_tokenizer(
            output_dir=tokenizer_dir,
            codeword_nums=codeword_nums
        )

    # å†æ¬¡ç¡®è®¤å…³é”®é…ç½®ï¼ˆåŒé‡ä¿é™©ï¼‰
    tokenizer.padding_side = "left"
    tokenizer.truncation_side = "left"
    if tokenizer.pad_token_id is None: 
        # å¦‚æœè¯»å–å‡ºæ¥çš„ tokenizer æ²¡è®°å½• pad_tokenï¼Œå°è¯•æ‰‹åŠ¨ä¿®å¤
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å°å¿ƒï¼Œä¸è¦è¦†ç›–äº†æ­£ç¡®çš„ IDï¼Œé€šå¸¸é‡å»ºæ—¶æ‰éœ€è¦è¿™æ­¥
        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("[PAD]")
    if tokenizer.pad_token_id is None: tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("[PAD]")
    if tokenizer.bos_token_id is None: tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids("<|endoftext|>")
    if tokenizer.eos_token_id is None: tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("<|endoftext|>")

    # 4. åŠ è½½æ¨¡å‹ Checkpoint
    logger.info(f"Loading Model from checkpoint: {args.checkpoint_path}")
    
    # åŠ¨æ€é€‰æ‹©æ¨¡å‹ç±»
    if args.model_name == 'sasrec':
        model_class = SasRecForCausalLM
    elif args.model_name == 'llama':
        model_class = LlamaForCausalLM
    elif args.model_name.startswith('qwen'):
        model_class = Qwen2ForCausalLM
    else:
        model_class = LlamaRecForCausalLM

    try:
        model = model_class.from_pretrained(
            args.checkpoint_path, 
            torch_dtype=torch.bfloat16 if config_data['training_args'].get('bf16', False) else "auto",
            device_map=device
        )
    except Exception as e:
        logger.error(f"Failed to load model from {args.checkpoint_path}. Error: {e}")
        # å°è¯•è‡ªåŠ¨å›é€€
        logger.info("Trying AutoModelForCausalLM...")
        model = AutoModelForCausalLM.from_pretrained(args.checkpoint_path, device_map=device)

    model.eval()

    # 5. å‡†å¤‡æ•°æ®
    logger.info(f"Loading dataset from {dataset_path}, split={args.split}")
    try:
        raw_dataset = load_dataset("json", data_dir=dataset_path, split=args.split)
    except Exception:
        logger.warning(f"Split '{args.split}' not found, falling back to 'train' just to test flow.")
        raw_dataset = load_dataset("json", data_dir=dataset_path, split='train')

    # Tokenize æ•°æ®
    logger.info("Tokenizing dataset...")
    eval_dataset = raw_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=8,
        load_from_cache_file=True, # åˆ©ç”¨ç¼“å­˜
        remove_columns=['prompt'], # ä¿ç•™ ground_truth
        fn_kwargs={"tokenizer": tokenizer, "max_seq_length": max_seq_length},
        desc="Tokenizing"
    )

    # Collator & DataLoader
    eval_collator = EvalDataCollator(tokenizer=tokenizer, max_length=max_seq_length)
    
    batch_size = args.batch_size if args.batch_size else training_args.get('per_device_eval_batch_size', 16)
    
    eval_dataloader = DataLoader(
        eval_dataset,
        batch_size=batch_size,
        collate_fn=eval_collator,
        shuffle=False,
        drop_last=False,
        num_workers=16
    )

    # 6. å‡†å¤‡ç”Ÿæˆå‚æ•°
    item_token_codebooks = build_item_token_codebooks_dynamically(tokenizer, generation_length)
    
    generation_config_params = {
        "generation_length": generation_length,
        "num_beams": testing_args['num_beams'],
        "k_values": testing_args['eval_k_values'],
        "item_token_codebooks": item_token_codebooks
    }

    # 7. å¼€å§‹è¯„ä¼°
    evaluator = StandaloneEvaluator(model, tokenizer, generation_config_params, device)
    metrics = evaluator.evaluate(eval_dataloader)

    # 8. è¾“å‡ºç»“æœ
    print("\n" + "="*30)
    print(" >>> Final Evaluation Results <<<")
    print("="*30)
    print(json.dumps(metrics, indent=4))
    print("="*30 + "\n")

    if args.output_file:
        with open(args.output_file, 'w') as f:
            json.dump(metrics, f, indent=4)
        logger.info(f"Metrics saved to {args.output_file}")

if __name__ == "__main__":
    main()