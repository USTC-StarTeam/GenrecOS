import os
import json
import logging
import yaml
import argparse
import sys
import tempfile
import warnings
from typing import List, Dict, Union
from datetime import datetime

import torch
from torch.utils.data import DataLoader
from transformers import (
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
    PreTrainedTokenizerFast,
    AddedToken,
    Qwen2Tokenizer,
    LogitsProcessorList,
)
import transformers.utils.logging
from datasets import load_dataset
from tqdm import tqdm
import numpy as np

# å¯¼å…¥ä½ çš„è‡ªå®šä¹‰æ¨¡å‹ä»£ç 
from llamarec import LlamaRecForCausalLM, LlamaRecConfig

# å¯¼å…¥åŒäº‹å†™çš„å·¥å…·ä»£ç 
from util.datacollator import TrainDataCollator, EvalDataCollator
from util.utils_evaluate import (
    build_item_token_codebooks_dynamically, 
    beamsearch_prefix_constraint_fn, 
    DynamicHierarchicalLogitsProcessor,
)
from util.eval import compute_hr_at_k, compute_ndcg_at_k

# å¿½ç•¥ç‰¹å®šçš„ FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning, module="transformers.trainer")

# çº¯å‡€ç‰ˆ Qwen Tokenizer æ„å»ºå‡½æ•°
def create_pure_id_qwen_tokenizer(
    output_dir: str, 
    codeword_nums: List[int]  # e.g., [100, 200, 400]
):
    """
    åŸºäº Qwen2Tokenizer æºç ï¼Œä»é›¶æ„å»ºä¸€ä¸ªçº¯å‡€çš„ã€åªåŒ…å«è¯­ä¹‰ ID çš„åˆ†è¯å™¨ã€‚
    """
    logging.info(f"Building Pure ID Qwen Tokenizer with codeword_nums={codeword_nums}...")
    
    # step 1: å‡†å¤‡ä¸€ä¸ªæç®€çš„ Dummy è¯è¡¨
    dummy_vocab = {"<|endoftext|>": 0}
    
    # ä½¿ç”¨ä¸´æ—¶ç›®å½•ç”Ÿæˆè¿™ä¸¤ä¸ªå¿…é¡»çš„æ–‡ä»¶
    with tempfile.TemporaryDirectory() as temp_dir:
        vocab_file = os.path.join(temp_dir, "vocab.json")
        merges_file = os.path.join(temp_dir, "merges.txt")
        
        with open(vocab_file, "w", encoding="utf-8") as f:
            json.dump(dummy_vocab, f)
        with open(merges_file, "w", encoding="utf-8") as f:
            f.write("#version: 0.2\n") 
            
        # step 2: åˆå§‹åŒ–åŸç”Ÿ Qwen2Tokenizer
        tokenizer = Qwen2Tokenizer(
            vocab_file=vocab_file,
            merges_file=merges_file,
            unk_token="<|endoftext|>",
            pad_token="<|endoftext|>",
            bos_token=None, 
            eos_token="<|endoftext|>",
        )

    # step 3: æ„å»ºä½ çš„è¯­ä¹‰ ID (AddedToken)
    new_tokens = []
    
    # æ¨èç³»ç»Ÿå¸¸ç”¨çš„æ§åˆ¶ç¬¦
    control_tokens = [
        AddedToken("[PAD]", special=True, normalized=False),
        AddedToken("[MASK]", special=True, normalized=False),
    ]
    new_tokens.extend(control_tokens)
    
    # ç”Ÿæˆè¯­ä¹‰ ID <a_0>, <b_10> ...
    for i, count in enumerate(codeword_nums):
        prefix = chr(ord('a') + i)
        for j in range(count):
            token_content = f"<{prefix}_{j}>"
            # æ ¸å¿ƒé…ç½®ï¼šspecial=True å¯ç”¨ Trie æ ‘è´ªå©ªåŒ¹é…ï¼Œè§£å†³æ— ç©ºæ ¼åˆ†è¯é—®é¢˜
            new_tokens.append(AddedToken(
                token_content, 
                special=True, 
                normalized=False, 
                lstrip=False, 
                rstrip=False
            ))

    # step 4: æ³¨å…¥ Token
    logging.info(f"Injecting {len(new_tokens)} semantic tokens into tokenizer...")
    tokenizer.add_special_tokens(
        {"additional_special_tokens": new_tokens}, 
        replace_additional_special_tokens=False
    )
    
    # æ›´æ–° pad_token_id
    if "[PAD]" in tokenizer.get_vocab():
        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("[PAD]")

    # step 5: ä¿å­˜ç»“æœ
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        
    tokenizer.save_pretrained(output_dir)
    logging.info(f"Tokenizer saved to: {output_dir}")
    logging.info(f"Final vocab size: {len(tokenizer)}")

    return tokenizer

# åŒ…å«ç”Ÿæˆå¼è¯„ä¼°çš„è®­ç»ƒæµç¨‹
class CustomTrainer(Trainer):
    def __init__(self, eval_collator, generation_config_params, **kwargs):
        super().__init__(**kwargs)
        self.eval_collator = eval_collator
        # å°†ç”Ÿæˆéœ€è¦çš„å‚æ•°å­˜ä¸‹æ¥
        self.gen_len = generation_config_params['generation_length']
        self.num_beams = generation_config_params['num_beams']
        self.k_values = generation_config_params['k_values']
        self.item_token_codebooks = generation_config_params['item_token_codebooks']

        # --- ã€æé€Ÿä¼˜åŒ–ã€‘æ„å»º NumPy å‘é‡åŒ–æŸ¥æ‰¾è¡¨ ---
        vocab = kwargs['processing_class'].get_vocab()
        
        # 1. æ‰¾åˆ°æœ€å¤§çš„ IDï¼Œç¡®å®šæ•°ç»„å¤§å°
        max_id = max(vocab.values())
        
        # 2. åˆå§‹åŒ–ä¸€ä¸ª object ç±»å‹çš„æ•°ç»„ï¼Œé»˜è®¤å¡«ç©ºå­—ç¬¦ä¸² ""
        # ä½¿ç”¨ dtype=object æ˜¯å› ä¸ºæˆ‘ä»¬çš„ token å­—ç¬¦ä¸²é•¿åº¦ä¸å›ºå®š
        self.vocab_array = np.array(["" for _ in range(max_id + 1)], dtype=object)
        
        # 3. å¡«å……æ•°ç»„ï¼šindex å°±æ˜¯ IDï¼Œvalue å°±æ˜¯ token å­—ç¬¦ä¸²
        for k, v in vocab.items():
            self.vocab_array[v] = k
            
        logging.info("âœ… NumPy Vectorized Vocab Lookup Table built.")

    # é‡å†™ evaluate æ–¹æ³•ä»¥æ”¯æŒç”ŸæˆæŒ‡æ ‡ (HR/NDCG)
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
        
        # 1. å‡†å¤‡ DataLoader
        eval_dataloader = DataLoader(
            eval_dataset,
            batch_size=self.args.eval_batch_size,
            collate_fn=self.eval_collator, 
            shuffle=False,
            drop_last=False
        )

        # 2. å‡†å¤‡æ¨¡å‹
        model = self._wrap_model(self.model, training=False, dataloader=eval_dataloader)
        model.eval()
        
        logging.info(f"***** Running Custom Evaluation (Generation) *****")
        
        total_metrics_sum = {f"HR@{k}": 0.0 for k in self.k_values}
        total_metrics_sum.update({f"NDCG@{k}": 0.0 for k in self.k_values})
        total_samples = 0

        
        # 3. å¾ªç¯ç”Ÿæˆ
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(eval_dataloader)): # åŠ ä¸ª enumerate æ–¹ä¾¿çœ‹æ˜¯ç¬¬å‡ ä¸ª batch
                # --- è®¡æ—¶èµ·ç‚¹ ---
                torch.cuda.synchronize()
                
                # 1. æ•°æ®ç§»åŠ¨
                input_ids = batch['input_ids'].to(self.args.device)
                attention_mask = batch['attention_mask'].to(self.args.device)
                groundtruth = batch['groundtruth'] # List[str]

                batch_size = input_ids.shape[0]
                prompt_length = input_ids.shape[1]

                torch.cuda.synchronize()

                # å®ä¾‹åŒ–æˆ‘ä»¬æ–°çš„ Processor
                # æ³¨æ„ï¼šå¿…é¡»æ”¾åœ¨å¾ªç¯é‡Œï¼Œå› ä¸º prompt_length å¯èƒ½ä¼šéš batch å˜åŒ–
                logits_processor = LogitsProcessorList([
                    DynamicHierarchicalLogitsProcessor(
                        prompt_length=prompt_length,
                        item_token_codebooks=self.item_token_codebooks,
                        device=self.args.device
                    )
                ])

                # 2. æ¨¡å‹ç”Ÿæˆ
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=prompt_length + self.gen_len,
                    num_beams=self.num_beams,
                    do_sample=False,
                    num_return_sequences=self.num_beams,
                    pad_token_id=self.processing_class.pad_token_id,
                    eos_token_id=self.processing_class.eos_token_id,
                    # prefix_allowed_tokens_fn=batch_beamsearch_prefix_constraint_fn,
                    logits_processor=logits_processor, 
                    use_cache=True
                )
                
                torch.cuda.synchronize() # ç­‰å¾… GPU ç”Ÿæˆå®Œæ¯•

                # # 3. è§£ç  (CPU å­—ç¬¦ä¸²æ“ä½œï¼Œå¦‚æœ Batch å¾ˆå¤§è¿™é‡Œä¼šæ…¢)
                # new_tokens = generated_ids[:, -self.gen_len:]
                # predicted_token_sequences = self.processing_class.batch_decode(new_tokens, skip_special_tokens=False)
                
                # 1. æ¬è¿åˆ° CPU å¹¶è½¬ä¸º numpy (O(1)è€—æ—¶)
                # shape: [Batch_Size * Num_Beams, Gen_Len]
                new_tokens_cpu = generated_ids[:, -self.gen_len:].cpu().numpy()
                
                # 2. å‘é‡åŒ–æŸ¥è¡¨ (Instant Lookup)
                # ç›´æ¥ç”¨ ID æ•°ç»„ä½œä¸ºç´¢å¼•ï¼Œç¬é—´å¾—åˆ°å¯¹åº”çš„å­—ç¬¦ä¸²æ•°ç»„
                # shape: [N, Gen_Len]ï¼Œå†…å®¹å˜æˆäº† ["<a_1>", "<b_2>", ...]
                token_strs = self.vocab_array[new_tokens_cpu]
                
                # 3. å‘é‡åŒ–æ‹¼æ¥ (Vectorized Join)
                # æ—¢ç„¶ Gen_Len é€šå¸¸å¾ˆçŸ­ï¼ˆæ¯”å¦‚3æˆ–4ï¼‰ï¼Œæˆ‘ä»¬ç›´æ¥æŒ‰åˆ—ç›¸åŠ 
                # NumPy çš„ object array æ”¯æŒç”¨ + å·è¿›è¡Œå­—ç¬¦ä¸²æ‹¼æ¥ï¼Œè¿™æ¯” Python å¾ªç¯å¿«å¾—å¤š
                
                if self.gen_len == 1:
                    predicted_token_sequences = token_strs.flatten().tolist()
                else:
                    # è¿™æ˜¯ä¸€ä¸ªç´¯åŠ è¿‡ç¨‹ï¼šCol0 + Col1 + Col2 ...
                    # æ¯”å¦‚ ["<a_1>"] + ["<b_1>"] = ["<a_1><b_1>"]
                    # è¿™ç§æ“ä½œæ˜¯åœ¨ C å±‚é¢å¾ªç¯çš„
                    result_array = token_strs[:, 0]
                    for i in range(1, self.gen_len):
                        result_array = result_array + token_strs[:, i]
                    
                    predicted_token_sequences = result_array.tolist()

                # 4. Reshape & æŒ‡æ ‡è®¡ç®— (çº¯ CPU é€»è¾‘)
                reshaped_token_sequences = [
                    predicted_token_sequences[i : i + self.num_beams]
                    for i in range(0, len(predicted_token_sequences), self.num_beams)
                ]

                batch_hr = compute_hr_at_k(reshaped_token_sequences, groundtruth, self.k_values)
                batch_ndcg = compute_ndcg_at_k(reshaped_token_sequences, groundtruth, self.k_values)

                # 5. ç´¯åŠ 
                for k_val in self.k_values:
                    total_metrics_sum[f"HR@{k_val}"] += batch_hr[f"HR@{k_val}"] * batch_size
                    total_metrics_sum[f"NDCG@{k_val}"] += batch_ndcg[f"NDCG@{k_val}"] * batch_size
                
                total_samples += batch_size

        # 4. æ±‡æ€»æŒ‡æ ‡
        metrics = {f"{metric_key_prefix}_{k}": (v / total_samples) for k, v in total_metrics_sum.items()}
        
        # 5. è®°å½•æ—¥å¿—
        self.log(metrics)
        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
        
        logging.info(f"Evaluation metrics: {metrics}")
        return metrics


# =============================================================================
# 3. æ•´åˆåçš„ Main å‡½æ•°
# =============================================================================
def main():
    # è·å–é…ç½®æ–‡ä»¶è·¯å¾„
    parser = argparse.ArgumentParser(description="Train a LlamaRec model using a YAML config file.")
    parser.add_argument("--dataset", type=str, default='Beauty')
    parser.add_argument("--model_name", type=str, default='llama-rec')
    args = parser.parse_args()

    # 1. ç¨³å¥çš„è·¯å¾„è¯»å–
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(current_script_dir, "pretrain_config", f"{args.dataset}_{args.model_name}.yaml")
    
    logging.info(f"Loading configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config_data = yaml.safe_load(f)

    # ä»è§£æçš„æ–‡ä»¶ä¸­æå–é…ç½®
    paths_config = config_data['paths']
    model_params = config_data['model_params']
    training_args_dict = config_data['training_args']
    tokenizer_params = config_data['tokenizer_params']
    testing_args = config_data['testing_args']

    # è·¯å¾„ä¸å‚æ•°å¤„ç†
    # å‡è®¾ dataset_path æŒ‡å‘åŒ…å« train.json/test.json çš„ç›®å½•
    dataset_path = paths_config['dataset_path'] 
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(paths_config['output_dir'], f"{model_params.get('MODEL_TYPE', 'model')}_{timestamp}")
    tokenizer_dir = paths_config['tokenizer_dir']
    max_seq_length = model_params['max_seq_length']
    # æ³¨æ„ï¼šcodebook_num å¯¹åº”çš„å°±æ˜¯ generation çš„é•¿åº¦ï¼ˆæ¯ä¸ª item æœ‰å‡ å±‚ IDï¼‰
    # è¿™é‡Œå‡è®¾ YAML é‡Œå†™çš„æ˜¯ codeword_nums åˆ—è¡¨ï¼Œcodebook_num æ˜¯åˆ—è¡¨é•¿åº¦
    codeword_nums = tokenizer_params.get('codeword_nums', [20, 20, 20])
    generation_length = len(codeword_nums)

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # ================= æ—¥å¿—é…ç½®åŒºåŸŸ =================
    log_file_path = os.path.join(output_dir, "training_process.log")
    
    file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(name)s -   %(message)s", datefmt="%m/%d/%Y %H:%M:%S")
    file_handler.setFormatter(formatter)
    
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.setFormatter(formatter)

    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    if root_logger.hasHandlers():
        root_logger.handlers.clear()
    root_logger.addHandler(file_handler)
    root_logger.addHandler(stream_handler)

    transformers.utils.logging.set_verbosity_info()
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    transformers_logger = transformers.utils.logging.get_logger("transformers")
    transformers_logger.addHandler(file_handler)
    
    logging.info(f"âœ… Logging started. Output file: {log_file_path}")
    # ==========================================================

    # ==========================================================
    # Tokenizer åˆ›å»º (æ›¿æ¢ä¸ºä½ çš„ create_pure_id_qwen_tokenizer)
    # ==========================================================
    # tokenizer_file = os.path.join(tokenizer_dir, "tokenizer.json")
    
    # é€»è¾‘ï¼šå¦‚æœæ²¡æœ‰ç°æˆçš„ jsonï¼Œæˆ–è€…ä¸ºäº†ä¿è¯é…ç½®ä¸€è‡´ï¼Œå»ºè®®ä½¿ç”¨ create_pure_id_qwen_tokenizer
    # å®ƒå†…éƒ¨æ˜¯åŸºäºå†…å­˜æ„å»ºçš„ Qwen2Tokenizerï¼Œéå¸¸è½»é‡
    
    # åªè¦ YAML é‡Œé…äº† codeword_numsï¼Œæˆ‘ä»¬å°±åŠ¨æ€æ„å»ºï¼Œç¡®ä¿ä¸€è‡´æ€§
    tokenizer = create_pure_id_qwen_tokenizer(
        output_dir=tokenizer_dir,
        codeword_nums=codeword_nums
    )

    # ä¿®æ”¹ä¸€ä¸‹tokenizerçš„paddingä½ç½®
    tokenizer.padding_side = "left"   # å¼ºåˆ¶è®¾ä¸ºå·¦å¡«å……
    tokenizer.truncation_side = "left" # (å¯é€‰) æˆªæ–­é€šå¸¸ä¹Ÿè®¾ä¸ºå·¦ä¾§ï¼Œä¿ç•™æœ€æ–°çš„å†å²

    # å¥å£®æ€§æ£€æŸ¥
    if tokenizer.pad_token_id is None: tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("[PAD]")
    # Qwen é»˜è®¤æ—  BOS/EOSï¼Œè¿™é‡Œç”¨ <|endoftext|> æˆ–è€…æˆ‘ä»¬åˆšåŠ çš„ [PAD] å…œåº•ï¼Œæˆ–è€…æ ¹æ®æ¨¡å‹é€»è¾‘æŒ‡å®š
    # å¦‚æœä½ çš„æ¨¡å‹ä¾èµ– BOS/EOS å¯åŠ¨/ç»“æŸï¼Œç¡®ä¿å®ƒä»¬å­˜åœ¨
    if tokenizer.bos_token_id is None: 
         # å¦‚æœè¯è¡¨é‡Œæ²¡ [BOS]ï¼Œç”¨ <|endoftext|> é¡¶æ›¿
        tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids("<|endoftext|>")
    if tokenizer.eos_token_id is None: 
        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("<|endoftext|>")

    logging.info(f"Final check - vocab: {len(tokenizer)}, pad: {tokenizer.pad_token_id}, bos: {tokenizer.bos_token_id}, eos: {tokenizer.eos_token_id}")

    # ==========================================================
    # æ•°æ®é›†åŠ è½½
    # ==========================================================
    # # å‡è®¾ç›®å½•ç»“æ„æ˜¯ train_data.json å’Œ test_data.json
    # data_files_train = os.path.join(dataset_path, 'train_data.json')
    # data_files_test = os.path.join(dataset_path, 'test_data.json') # æˆ–è€…æ˜¯ valid
    
    # ä½¿ç”¨ data_files å‚æ•°åŠ è½½æŒ‡å®šæ–‡ä»¶
    train_dataset = load_dataset("json", data_dir=dataset_path, split='train')
    # å¦‚æœæ²¡æœ‰å•ç‹¬çš„ test æ–‡ä»¶ï¼Œç”¨ train åˆ‡åˆ†æˆ–è€…æ€æ ·ï¼Œè¿™é‡Œå‡è®¾æœ‰
    # åŒäº‹ä»£ç é‡Œç”¨çš„ä¹Ÿæ˜¯ data_files=dataset_pathï¼ˆå¯èƒ½æ˜¯ä¸ªåŒ…å«å¤šä¸ªjsonçš„ç›®å½•ï¼Ÿï¼‰ï¼Œè¿™é‡ŒæŒ‰æ ‡å‡†å†™æ³•
    try:
        eval_dataset = load_dataset("json", data_dir=dataset_path, split='valid')
    except:
        logging.warning("Test file not found, using train set as eval (FOR DEBUG ONLY)")
        eval_dataset = train_dataset

    # ==========================================================
    # æ¨¡å‹æ„å»º
    # ==========================================================
    logging.info("Creating model from scratch...")
    
    config = LlamaRecConfig(
        # æ ¸å¿ƒï¼šå°† Tokenizer çš„å®é™…å¤§å°ä¼ ç»™ Config
        vocab_size=len(tokenizer),
        
        hidden_size=model_params['hidden_size'],
        intermediate_size=model_params['intermediate_size'],
        num_hidden_layers=model_params['num_hidden_layers'],
        num_attention_heads=model_params['num_attention_heads'],
        # åºåˆ—é•¿åº¦ = Prompt é•¿åº¦ + ç”Ÿæˆé•¿åº¦
        max_position_embeddings=max_seq_length + generation_length,
        rms_norm_eps=model_params['rms_norm_eps'],
        model_type=model_params.get('MODEL_TYPE', 'llama-rec'),
        use_cache=False,
        pad_token_id=tokenizer.pad_token_id,
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        
        # è¿™äº›æ˜¯ LlamaRec ç‰¹æœ‰çš„å‚æ•°ï¼Œå¦‚æœ config é‡Œæœ‰å°±ä¼ 
        # num_levels=len(codeword_nums), 
    )
    model = LlamaRecForCausalLM(config)
    logging.info(f"Model created with {model.num_parameters() / 1e6:.2f} M parameters.")

    # ==========================================================
    # Trainer å‡†å¤‡
    # ==========================================================
    training_args_dict['output_dir'] = output_dir
    training_args_dict['logging_dir'] = os.path.join(output_dir, 'logs')
    training_args = TrainingArguments(**training_args_dict)

    # DataCollator
    # æ³¨æ„ï¼šç¡®ä¿ Collator é‡Œçš„ tokenizer è°ƒç”¨å‚æ•°æ˜¯æ­£ç¡®çš„ï¼ˆis_split_into_words=Falseï¼‰
    train_collator = TrainDataCollator(tokenizer=tokenizer, max_length=max_seq_length)
    eval_collator = EvalDataCollator(tokenizer=tokenizer, max_length=max_seq_length)

    # åŠ¨æ€æ„å»º Codebooks (ç”¨äºç”Ÿæˆçº¦æŸ)
    # è¿™éœ€è¦åˆ©ç”¨ä½ çš„ tokenizer æ¥è§£æ <a_0> å¯¹åº”çš„ ID
    item_token_codebooks = build_item_token_codebooks_dynamically(tokenizer, generation_length)
    
    generation_config_params = {
        "generation_length": generation_length,
        "num_beams": testing_args['num_beams'],
        "k_values": testing_args['eval_k_values'],
        "item_token_codebooks": item_token_codebooks
    }

    # å®ä¾‹åŒ– CustomTrainer
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        processing_class=tokenizer, # ä¼ å…¥ tokenizer å¯¹è±¡
        data_collator=train_collator,
        eval_collator=eval_collator,
        generation_config_params=generation_config_params,
        # æ—©åœç­–ç•¥
        callbacks=[EarlyStoppingCallback(early_stopping_patience=testing_args['early_stopping_patience'])] 
    )

    # ==========================================================
    # è®­ç»ƒä¸ä¿å­˜
    # ==========================================================
    logging.info("Starting training...")
    trainer.train()

    # æ‰“å°æœ€ä¼˜ç»“æœ
    if trainer.state.best_model_checkpoint:
        best_metric = training_args.metric_for_best_model
        logging.info("=" * 40)
        logging.info(f"ğŸ† Best Model Checkpoint: {trainer.state.best_model_checkpoint}")
        logging.info(f"Best Metric ({best_metric}): {trainer.state.best_metric}")
        logging.info("=" * 40)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ (Best Model)
    # å¦‚æœ load_best_model_at_end=Trueï¼Œæ­¤æ—¶ model å·²ç»æ˜¯æœ€å¥½çš„äº†
    final_model_path = os.path.join(output_dir, "best_model")
    logging.info(f"Saving best model to {final_model_path}")
    
    trainer.save_model(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    
    logging.info("All operations complete!")

if __name__ == "__main__":
    main()