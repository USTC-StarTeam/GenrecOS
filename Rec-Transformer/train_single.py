import os
import json
import logging
import yaml
import argparse
import sys
import tempfile
import warnings
from typing import List
import random
from datetime import datetime

import torch
from torch.utils.data import DataLoader
from transformers import (
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
    PreTrainedTokenizerFast,
    AddedToken,
    Qwen2Tokenizer,
    LogitsProcessorList,
    LlamaForCausalLM, 
    LlamaConfig,
    Qwen2ForCausalLM,
    Qwen2Config,
)
import transformers.utils.logging
from datasets import load_dataset
from tqdm import tqdm
import numpy as np

# å¯¼å…¥ä½ çš„è‡ªå®šä¹‰æ¨¡å‹ä»£ç 
from llamarec import LlamaRecForCausalLM, LlamaRecConfig
from sasrec import SasRecForCausalLM, SasRecConfig

sys.path.append("../")
# å¯¼å…¥åŒäº‹å†™çš„å·¥å…·ä»£ç 
from utils.datacollator import TrainDataCollator, EvalDataCollator, preprocess_function
from utils.utils_evaluate import (
    build_item_token_codebooks_dynamically, 
    beamsearch_prefix_constraint_fn, 
    DynamicHierarchicalLogitsProcessor,
)
from utils.eval import compute_hr_at_k, compute_ndcg_at_k

# å¿½ç•¥ç‰¹å®šçš„ FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning, module="transformers.trainer")

# çº¯å‡€ç‰ˆ Qwen Tokenizer æ„å»ºå‡½æ•°
def create_pure_id_qwen_tokenizer(
    output_dir: str, 
    codeword_nums: List[int]  # e.g., [100, 200, 400]
):
    """
    åŸºäº Qwen2Tokenizer æºç ï¼Œä»é›¶æ„å»ºä¸€ä¸ªçº¯å‡€çš„ã€åªåŒ…å«è¯­ä¹‰ ID çš„åˆ†è¯å™¨ã€‚
    """
    logging.info(f"Building Pure ID Qwen Tokenizer with codeword_nums={codeword_nums}...")
    
    # step 1: å‡†å¤‡ä¸€ä¸ªæç®€çš„ Dummy è¯è¡¨
    dummy_vocab = {"<|endoftext|>": 0}
    
    # ä½¿ç”¨ä¸´æ—¶ç›®å½•ç”Ÿæˆè¿™ä¸¤ä¸ªå¿…é¡»çš„æ–‡ä»¶
    with tempfile.TemporaryDirectory() as temp_dir:
        vocab_file = os.path.join(temp_dir, "vocab.json")
        merges_file = os.path.join(temp_dir, "merges.txt")
        
        with open(vocab_file, "w", encoding="utf-8") as f:
            json.dump(dummy_vocab, f)
        with open(merges_file, "w", encoding="utf-8") as f:
            f.write("#version: 0.2\n") 
            
        # step 2: åˆå§‹åŒ–åŸç”Ÿ Qwen2Tokenizer
        tokenizer = Qwen2Tokenizer(
            vocab_file=vocab_file,
            merges_file=merges_file,
            unk_token="<|endoftext|>",
            pad_token="<|endoftext|>",
            bos_token=None, 
            eos_token="<|endoftext|>",
        )

    # step 3: æ„å»ºä½ çš„è¯­ä¹‰ ID (AddedToken)
    new_tokens = []
    
    # æ¨èç³»ç»Ÿå¸¸ç”¨çš„æ§åˆ¶ç¬¦
    control_tokens = [
        AddedToken("[PAD]", special=True, normalized=False),
        AddedToken("[MASK]", special=True, normalized=False),
    ]
    new_tokens.extend(control_tokens)
    
    # ç”Ÿæˆè¯­ä¹‰ ID <a_0>, <b_10> ...
    for i, count in enumerate(codeword_nums):
        prefix = chr(ord('a') + i)
        for j in range(count):
            token_content = f"<{prefix}_{j}>"
            # æ ¸å¿ƒé…ç½®ï¼šspecial=True å¯ç”¨ Trie æ ‘è´ªå©ªåŒ¹é…ï¼Œè§£å†³æ— ç©ºæ ¼åˆ†è¯é—®é¢˜
            new_tokens.append(AddedToken(
                token_content, 
                special=True, 
                normalized=False, 
                lstrip=False, 
                rstrip=False
            ))

    # step 4: æ³¨å…¥ Token
    logging.info(f"Injecting {len(new_tokens)} semantic tokens into tokenizer...")
    tokenizer.add_special_tokens(
        {"additional_special_tokens": new_tokens}, 
        replace_additional_special_tokens=False
    )
    
    # æ›´æ–° pad_token_id
    if "[PAD]" in tokenizer.get_vocab():
        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("[PAD]")

    # step 5: ä¿å­˜ç»“æœ
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        
    tokenizer.save_pretrained(output_dir)
    logging.info(f"Tokenizer saved to: {output_dir}")
    logging.info(f"Final vocab size: {len(tokenizer)}")

    return tokenizer

# åŒ…å«ç”Ÿæˆå¼è¯„ä¼°çš„è®­ç»ƒæµç¨‹
class CustomTrainer(Trainer):
    def __init__(self, eval_collator, generation_config_params, **kwargs):
        super().__init__(**kwargs)
        self.eval_collator = eval_collator
        # å°†ç”Ÿæˆéœ€è¦çš„å‚æ•°å­˜ä¸‹æ¥
        self.gen_len = generation_config_params['generation_length']
        self.num_beams = generation_config_params['num_beams']
        self.k_values = generation_config_params['k_values']
        self.item_token_codebooks = generation_config_params['item_token_codebooks']

        # --- ã€æé€Ÿä¼˜åŒ–ã€‘æ„å»º NumPy å‘é‡åŒ–æŸ¥æ‰¾è¡¨ ---
        vocab = kwargs['processing_class'].get_vocab()
        
        # 1. æ‰¾åˆ°æœ€å¤§çš„ IDï¼Œç¡®å®šæ•°ç»„å¤§å°
        max_id = max(vocab.values())
        
        # 2. åˆå§‹åŒ–ä¸€ä¸ª object ç±»å‹çš„æ•°ç»„ï¼Œé»˜è®¤å¡«ç©ºå­—ç¬¦ä¸² ""
        # ä½¿ç”¨ dtype=object æ˜¯å› ä¸ºæˆ‘ä»¬çš„ token å­—ç¬¦ä¸²é•¿åº¦ä¸å›ºå®š
        self.vocab_array = np.array(["" for _ in range(max_id + 1)], dtype=object)
        
        # 3. å¡«å……æ•°ç»„ï¼šindex å°±æ˜¯ IDï¼Œvalue å°±æ˜¯ token å­—ç¬¦ä¸²
        for k, v in vocab.items():
            self.vocab_array[v] = k
            
        logging.info("âœ… NumPy Vectorized Vocab Lookup Table built.")

    # é‡å†™ evaluate æ–¹æ³•ä»¥æ”¯æŒç”ŸæˆæŒ‡æ ‡ (HR/NDCG)
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        # 1. è·å–ç›®æ ‡æ•°æ®é›†
        # å¦‚æœè°ƒç”¨æ—¶æ²¡ä¼  datasetï¼Œå°±ç”¨ Trainer è‡ªå¸¦çš„éªŒè¯é›†
        target_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
        
        # 2. ã€å…³é”®ä¿®æ”¹ã€‘åˆ¤æ–­æ˜¯å¦éœ€è¦é‡‡æ ·
        # é€»è¾‘ï¼šåªæœ‰å½“ metric_key_prefix ä¸º "eval" (è®­ç»ƒä¸­çš„éªŒè¯) ä¸”æ•°æ®é‡å¤§äº 1000 æ—¶æ‰é‡‡æ ·
        # å¦‚æœæ˜¯ "test" (æœ€åçš„ä¸»å‡½æ•°è°ƒç”¨)ï¼Œåˆ™ä¸é‡‡æ ·ï¼Œè·‘å…¨é‡
        eval_sample_num = 8000  # ä½ æƒ³è¦çš„é‡‡æ ·æ•°é‡
        
        if metric_key_prefix == "eval" and target_dataset is not None:
            total_size = len(target_dataset)
            if total_size > eval_sample_num:
                logging.info(f"âš¡ [SpeedUp] Sampling {eval_sample_num} random examples from {total_size} for validation.")
                
                # éšæœºé€‰å–ç´¢å¼•
                # æ³¨æ„ï¼šè¿™é‡Œæ¯æ¬¡éªŒè¯éƒ½ä¼šé‡æ–°éšæœºï¼Œå¯¼è‡´éªŒè¯æŒ‡æ ‡ä¼šæœ‰æ³¢åŠ¨ï¼Œä½†èƒ½æ›´å…¨é¢åœ°ç›‘æ§æ¨¡å‹
                random_indices = random.sample(range(total_size), eval_sample_num)
                
                # ä½¿ç”¨ HuggingFace dataset çš„ select æ–¹æ³•åˆ›å»ºå­é›†
                target_dataset = target_dataset.select(random_indices)
            else:
                logging.info(f"Dataset size ({total_size}) <= {eval_sample_num}, running full evaluation.")

        # 3. å‡†å¤‡ DataLoader (æ³¨æ„è¿™é‡Œè¦æŠŠ dataset æ¢æˆ target_dataset)
        eval_dataloader = DataLoader(
            target_dataset,  # ä½¿ç”¨å¤„ç†åçš„æ•°æ®é›†
            batch_size=self.args.eval_batch_size,
            collate_fn=self.eval_collator, 
            shuffle=False,
            drop_last=False
        )

        # 4. å‡†å¤‡æ¨¡å‹
        model = self._wrap_model(self.model, training=False, dataloader=eval_dataloader)
        model.eval()
        
        logging.info(f"***** Running Custom Evaluation ({metric_key_prefix}) *****")
        logging.info(f"  Num examples = {len(target_dataset)}")
        logging.info(f"  Batch size = {self.args.eval_batch_size}")
        
        total_metrics_sum = {f"HR@{k}": 0.0 for k in self.k_values}
        total_metrics_sum.update({f"NDCG@{k}": 0.0 for k in self.k_values})
        total_samples = 0

        # 5. å¾ªç¯ç”Ÿæˆ (ä¿æŒä¸å˜)
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(eval_dataloader, desc=f"Evaluating ({metric_key_prefix})")):
                input_ids = batch['input_ids'].to(self.args.device)
                attention_mask = batch['attention_mask'].to(self.args.device)
                groundtruth = batch['groundtruth']

                batch_size = input_ids.shape[0]
                prompt_length = input_ids.shape[1]

                logits_processor = LogitsProcessorList([
                    DynamicHierarchicalLogitsProcessor(
                        prompt_length=prompt_length,
                        item_token_codebooks=self.item_token_codebooks,
                        device=self.args.device
                    )
                ])

                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=prompt_length + self.gen_len,
                    num_beams=self.num_beams,
                    do_sample=False,
                    num_return_sequences=self.num_beams,
                    pad_token_id=self.processing_class.pad_token_id,
                    eos_token_id=self.processing_class.eos_token_id,
                    logits_processor=logits_processor, 
                    use_cache=True
                )
                
                # å‘é‡åŒ–è§£ç ä¸æ‹¼æ¥
                new_tokens_cpu = generated_ids[:, -self.gen_len:].cpu().numpy()
                token_strs = self.vocab_array[new_tokens_cpu]
                
                if self.gen_len == 1:
                    predicted_token_sequences = token_strs.flatten().tolist()
                else:
                    result_array = token_strs[:, 0]
                    for i in range(1, self.gen_len):
                        result_array = result_array + token_strs[:, i]
                    predicted_token_sequences = result_array.tolist()

                reshaped_token_sequences = [
                    predicted_token_sequences[i : i + self.num_beams]
                    for i in range(0, len(predicted_token_sequences), self.num_beams)
                ]

                batch_hr = compute_hr_at_k(reshaped_token_sequences, groundtruth, self.k_values)
                batch_ndcg = compute_ndcg_at_k(reshaped_token_sequences, groundtruth, self.k_values)

                for k_val in self.k_values:
                    total_metrics_sum[f"HR@{k_val}"] += batch_hr[f"HR@{k_val}"] * batch_size
                    total_metrics_sum[f"NDCG@{k_val}"] += batch_ndcg[f"NDCG@{k_val}"] * batch_size
                
                total_samples += batch_size

        # 6. æ±‡æ€»æŒ‡æ ‡
        # é˜²æ­¢é™¤ä»¥0
        if total_samples == 0:
            metrics = {f"{metric_key_prefix}_{k}": 0.0 for k in total_metrics_sum.keys()}
        else:
            metrics = {f"{metric_key_prefix}_{k}": (v / total_samples) for k, v in total_metrics_sum.items()}
        
        self.log(metrics)
        # è§¦å‘ Trainer çš„å›è°ƒï¼ˆæ¯”å¦‚ EarlyStoppingï¼‰
        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
        
        logging.info(f"Evaluation metrics: {metrics}")
        return metrics


# =============================================================================
# 3. æ•´åˆåçš„ Main å‡½æ•°
# =============================================================================
def main():
    # è·å–é…ç½®æ–‡ä»¶è·¯å¾„
    parser = argparse.ArgumentParser(description="Train a LlamaRec model using a YAML config file.")
    parser.add_argument("--dataset", type=str, default='Beauty')
    parser.add_argument("--model_name", type=str, default='llamarec')
    args = parser.parse_args()

    # 1. ç¨³å¥çš„è·¯å¾„è¯»å–
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(current_script_dir, "pretrain_config", args.dataset, f"{args.model_name}.yaml")
    
    logging.info(f"Loading configuration from: {config_path}")
    with open(config_path, 'r') as f:
        config_data = yaml.safe_load(f)

    # ä»è§£æçš„æ–‡ä»¶ä¸­æå–é…ç½®
    paths_config = config_data['paths']
    model_params = config_data['model_params']
    training_args_dict = config_data['training_args']
    tokenizer_params = config_data['tokenizer_params']
    testing_args = config_data['testing_args']

    # è·¯å¾„ä¸å‚æ•°å¤„ç†
    # å‡è®¾ dataset_path æŒ‡å‘åŒ…å« train.json/test.json çš„ç›®å½•
    dataset_path = paths_config['dataset_path'] 
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(paths_config['output_dir'], f"{model_params.get('MODEL_TYPE', 'model')}_{timestamp}")
    tokenizer_dir = paths_config['tokenizer_dir']
    max_seq_length = model_params['max_seq_length']
    # æ³¨æ„ï¼šcodebook_num å¯¹åº”çš„å°±æ˜¯ generation çš„é•¿åº¦ï¼ˆæ¯ä¸ª item æœ‰å‡ å±‚ IDï¼‰
    # è¿™é‡Œå‡è®¾ YAML é‡Œå†™çš„æ˜¯ codeword_nums åˆ—è¡¨ï¼Œcodebook_num æ˜¯åˆ—è¡¨é•¿åº¦
    codeword_nums = tokenizer_params.get('codeword_nums', [20, 20, 20])
    generation_length = len(codeword_nums)

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # ================= æ—¥å¿—é…ç½®åŒºåŸŸ =================
    log_file_path = os.path.join(output_dir, "training_process.log")
    
    file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(name)s -   %(message)s", datefmt="%m/%d/%Y %H:%M:%S")
    file_handler.setFormatter(formatter)
    
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.setFormatter(formatter)

    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    if root_logger.hasHandlers():
        root_logger.handlers.clear()
    root_logger.addHandler(file_handler)
    root_logger.addHandler(stream_handler)

    transformers.utils.logging.set_verbosity_info()
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()
    transformers_logger = transformers.utils.logging.get_logger("transformers")
    transformers_logger.addHandler(file_handler)
    
    logging.info(f"âœ… Logging started. Output file: {log_file_path}")
    # ==========================================================

    # ==========================================================
    # Tokenizer åˆ›å»º (æ›¿æ¢ä¸ºä½ çš„ create_pure_id_qwen_tokenizer)
    # ==========================================================
    # tokenizer_file = os.path.join(tokenizer_dir, "tokenizer.json")
    
    # é€»è¾‘ï¼šå¦‚æœæ²¡æœ‰ç°æˆçš„ jsonï¼Œæˆ–è€…ä¸ºäº†ä¿è¯é…ç½®ä¸€è‡´ï¼Œå»ºè®®ä½¿ç”¨ create_pure_id_qwen_tokenizer
    # å®ƒå†…éƒ¨æ˜¯åŸºäºå†…å­˜æ„å»ºçš„ Qwen2Tokenizerï¼Œéå¸¸è½»é‡
    
    # åªè¦ YAML é‡Œé…äº† codeword_numsï¼Œæˆ‘ä»¬å°±åŠ¨æ€æ„å»ºï¼Œç¡®ä¿ä¸€è‡´æ€§
    tokenizer = create_pure_id_qwen_tokenizer(
        output_dir=tokenizer_dir,
        codeword_nums=codeword_nums
    )

    # ä¿®æ”¹ä¸€ä¸‹tokenizerçš„paddingä½ç½®
    tokenizer.padding_side = "left"   # å¼ºåˆ¶è®¾ä¸ºå·¦å¡«å……
    tokenizer.truncation_side = "left" # (å¯é€‰) æˆªæ–­é€šå¸¸ä¹Ÿè®¾ä¸ºå·¦ä¾§ï¼Œä¿ç•™æœ€æ–°çš„å†å²

    # å¥å£®æ€§æ£€æŸ¥
    if tokenizer.pad_token_id is None: tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("[PAD]")
    # Qwen é»˜è®¤æ—  BOS/EOSï¼Œè¿™é‡Œç”¨ <|endoftext|> æˆ–è€…æˆ‘ä»¬åˆšåŠ çš„ [PAD] å…œåº•ï¼Œæˆ–è€…æ ¹æ®æ¨¡å‹é€»è¾‘æŒ‡å®š
    # å¦‚æœä½ çš„æ¨¡å‹ä¾èµ– BOS/EOS å¯åŠ¨/ç»“æŸï¼Œç¡®ä¿å®ƒä»¬å­˜åœ¨
    if tokenizer.bos_token_id is None: 
         # å¦‚æœè¯è¡¨é‡Œæ²¡ [BOS]ï¼Œç”¨ <|endoftext|> é¡¶æ›¿
        tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids("<|endoftext|>")
    if tokenizer.eos_token_id is None: 
        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("<|endoftext|>")

    logging.info(f"Final check - vocab: {len(tokenizer)}, pad: {tokenizer.pad_token_id}, bos: {tokenizer.bos_token_id}, eos: {tokenizer.eos_token_id}")

    # ==========================================================
    # æ•°æ®é›†åŠ è½½
    # ==========================================================
    # # å‡è®¾ç›®å½•ç»“æ„æ˜¯ train_data.json å’Œ test_data.json
    # data_files_train = os.path.join(dataset_path, 'train_data.json')
    # data_files_test = os.path.join(dataset_path, 'test_data.json') # æˆ–è€…æ˜¯ valid
    
    # ä½¿ç”¨ data_files å‚æ•°åŠ è½½æŒ‡å®šæ–‡ä»¶
    train_dataset = load_dataset("json", data_dir=dataset_path, split='train')
    valid_dataset = train_dataset
    # å¦‚æœæ²¡æœ‰å•ç‹¬çš„ test æ–‡ä»¶ï¼Œç”¨ train åˆ‡åˆ†æˆ–è€…æ€æ ·ï¼Œè¿™é‡Œå‡è®¾æœ‰
    # åŒäº‹ä»£ç é‡Œç”¨çš„ä¹Ÿæ˜¯ data_files=dataset_pathï¼ˆå¯èƒ½æ˜¯ä¸ªåŒ…å«å¤šä¸ªjsonçš„ç›®å½•ï¼Ÿï¼‰ï¼Œè¿™é‡ŒæŒ‰æ ‡å‡†å†™æ³•
    try:
        eval_dataset = load_dataset("json", data_dir=dataset_path, split='test')
    except:
        logging.warning("Test file not found, using train set as eval!")
        eval_dataset = train_dataset

    # ==========================================================
    # æ¨¡å‹æ„å»º
    # ==========================================================
    logging.info(f"Creating model ({args.model_name}) from scratch...")

    if args.model_name == 'sasrec':
        config_class = SasRecConfig
        model_class = SasRecForCausalLM
    elif args.model_name == 'llama':
        # é»˜è®¤ä¸º llamarec
        config_class = LlamaConfig
        model_class = LlamaForCausalLM
    elif args.model_name.startswith('qwen'):  # æ”¯æŒ qwen, qwen2, qwen2.5
        # === æ–°å¢ Qwen åˆ†æ”¯ ===
        config_class = Qwen2Config
        model_class = Qwen2ForCausalLM
    else:
        # é»˜è®¤ä¸º llamarec
        config_class = LlamaRecConfig
        model_class = LlamaRecForCausalLM

    # æ„å»ºconfig
    dynamic_args = {
        "vocab_size": len(tokenizer),
        "max_position_embeddings": max_seq_length + generation_length,
        "model_type": model_params.get('MODEL_TYPE', args.model_name),
        "use_cache": False,
        "pad_token_id": tokenizer.pad_token_id,
        "bos_token_id": tokenizer.bos_token_id,
        "eos_token_id": tokenizer.eos_token_id,
    }
    config_kwargs = model_params.copy()
    config_kwargs.update(dynamic_args)
    config_kwargs.pop('MODEL_TYPE', None) 

    config = config_class(**config_kwargs)
    model = model_class(config)

    logging.info(f"Model created with {model.num_parameters() / 1e6:.2f} M parameters.")

    # ==========================================================
    # Trainer å‡†å¤‡
    # ==========================================================
    training_args_dict['output_dir'] = output_dir
    training_args_dict['logging_dir'] = os.path.join(output_dir, 'logs')
    training_args = TrainingArguments(**training_args_dict)

    logging.info("â³ Pre-tokenizing dataset (this happens only once)...")
    # ä½¿ç”¨å¤šè¿›ç¨‹é¢„å¤„ç†ï¼Œé€Ÿåº¦é£å¿«
    # load_from_cache_file=True ä¼šè‡ªåŠ¨ç¼“å­˜ç»“æœï¼Œç¬¬äºŒæ¬¡è¿è¡Œç›´æ¥è¯»ç¡¬ç›˜ï¼Œæ— éœ€ç­‰å¾…
    train_dataset = train_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=training_args_dict['dataloader_num_workers'], # ä½¿ç”¨ 8 ä¸ªæ ¸å¹¶è¡Œå¤„ç†
        load_from_cache_file=True,    
        fn_kwargs={
            "tokenizer": tokenizer, 
            "max_seq_length": max_seq_length
        },
        remove_columns=["prompt", 'ground_truth'],   # ä¿ç•™ groundtruth ç­‰ä½ éœ€è¦ç”¨çš„åˆ—ï¼
        desc="Running tokenizer on train dataset",
    )
    valid_dataset = valid_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=training_args_dict['dataloader_num_workers'],
        load_from_cache_file=True,
        remove_columns=['prompt'],
        fn_kwargs={"tokenizer": tokenizer, "max_seq_length": max_seq_length},
        desc="Tokenizing valid set"
    )
    
    # å¯¹ eval_dataset ä¹ŸåšåŒæ ·çš„æ“ä½œ
    if eval_dataset:
        eval_dataset = eval_dataset.map(
            preprocess_function,
            batched=True,
            num_proc=training_args_dict['dataloader_num_workers'],
            load_from_cache_file=True,      
            remove_columns=['prompt'],
            fn_kwargs={"tokenizer": tokenizer, "max_seq_length": max_seq_length},
            desc="Tokenizing eval set"
        )

    # DataCollator
    # æ³¨æ„ï¼šç¡®ä¿ Collator é‡Œçš„ tokenizer è°ƒç”¨å‚æ•°æ˜¯æ­£ç¡®çš„ï¼ˆis_split_into_words=Falseï¼‰
    train_collator = TrainDataCollator(tokenizer=tokenizer, max_length=max_seq_length)
    eval_collator = EvalDataCollator(tokenizer=tokenizer, max_length=max_seq_length)

    # åŠ¨æ€æ„å»º Codebooks (ç”¨äºç”Ÿæˆçº¦æŸ)
    # è¿™éœ€è¦åˆ©ç”¨ä½ çš„ tokenizer æ¥è§£æ <a_0> å¯¹åº”çš„ ID
    item_token_codebooks = build_item_token_codebooks_dynamically(tokenizer, generation_length)
    
    generation_config_params = {
        "generation_length": generation_length,
        "num_beams": testing_args['num_beams'],
        "k_values": testing_args['eval_k_values'],
        "item_token_codebooks": item_token_codebooks
    }

    # å®ä¾‹åŒ– CustomTrainer
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        processing_class=tokenizer, # ä¼ å…¥ tokenizer å¯¹è±¡
        data_collator=train_collator,
        eval_collator=eval_collator,
        generation_config_params=generation_config_params,
        # æ—©åœç­–ç•¥
        callbacks=[EarlyStoppingCallback(early_stopping_patience=testing_args['early_stopping_patience'])] 
    )

    # ==========================================================
    # è®­ç»ƒä¸ä¿å­˜
    # ==========================================================
    logging.info("Starting training...")
    trainer.train()

    # æ‰“å°æœ€ä¼˜ç»“æœ
    if trainer.state.best_model_checkpoint:
        best_metric = training_args.metric_for_best_model
        logging.info("=" * 40)
        logging.info(f"ğŸ† Best Model Checkpoint: {trainer.state.best_model_checkpoint}")
        logging.info(f"Best Metric ({best_metric}): {trainer.state.best_metric}")
        logging.info("=" * 40)
    
    # ==========================================================
    # 4. æœ€ç»ˆæµ‹è¯• (Final Evaluation on Test Set)
    # ==========================================================
    logging.info("Starting Final Evaluation on the Test Set (using Best Model)...")

    # æ˜¾å¼è°ƒç”¨ evaluateï¼Œä¼ å…¥ eval_dataset (å³åŠ è½½çš„ test split)
    # metric_key_prefix="test" ä¼šè®©è¾“å‡ºçš„æŒ‡æ ‡å˜æˆ "test_HR@10" è€Œä¸æ˜¯ "eval_HR@10"ï¼Œæ–¹ä¾¿åŒºåˆ†
    test_metrics = trainer.evaluate(eval_dataset=eval_dataset, metric_key_prefix="test")

    # å°†æµ‹è¯•ç»“æœä¿å­˜åˆ°å•ç‹¬çš„ JSON æ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­è¯»å–
    test_results_path = os.path.join(output_dir, "test_results.json")
    with open(test_results_path, "w") as f:
        json.dump(test_metrics, f, indent=4)
    
    logging.info(f"Test results saved to {test_results_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ (Best Model)
    # å¦‚æœ load_best_model_at_end=Trueï¼Œæ­¤æ—¶ model å·²ç»æ˜¯æœ€å¥½çš„äº†
    final_model_path = os.path.join(output_dir, "best_model")
    logging.info(f"Saving best model to {final_model_path}")
    
    trainer.save_model(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    
    logging.info("All operations complete!")

if __name__ == "__main__":
    main()