expanded_model_path: "../models/expanded_model"
train_data_path: "../../Data/Beauty/FT_data/beauty_processed/align_data/train.json"
val_data_path: "../../Data/Beauty/FT_data/beauty_processed/align_data/val.json"
special_tokens_path: "../../Data/Beauty/FT_data/beauty/special_tokens.json"

# Global_Batch_Size = num_gpus * per_device_train_batch_size * gradient_accumulation_steps.
# Keep Global_Batch_Size = 8
per_device_train_batch_size: 2
gradient_accumulation_steps: 2 
per_device_eval_batch_size: 2

# 必须添加：防止DDP因为大部分参数被冻结而报错
ddp_find_unused_parameters: False 

num_train_epochs: 15
gradient_checkpointing: True
bf16: True
deepspeed: ./ds_zero2.json
output_dir: ../models/align_model
logging_dir: ../logs/train_align_log
logging_steps: 10
eval_strategy: epoch
eval_on_start: False
save_strategy: epoch
save_total_limit: 15
metric_for_best_model: eval_loss
greater_is_better: False
load_best_model_at_end: True
optim: adamw_torch
learning_rate: 1.0e-4
warmup_ratio: 0.0
weight_decay: 0.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8
max_grad_norm: 1.0
dataloader_num_workers: 4
remove_unused_columns: False