# train_grpo.py
import sys
import os
import yaml
import argparse
from datetime import datetime
import torch
from datasets import load_dataset
from tqdm import tqdm
from trl import GRPOConfig
from transformers import (
    AutoConfig, 
    AutoModelForCausalLM, 
    AutoTokenizer, 
    LogitsProcessorList, 
    TrainerCallback, 
    EarlyStoppingCallback,
    LlamaForCausalLM,
    Qwen2ForCausalLM,
)
from torch.utils.data import DataLoader
import numpy as np
import random
import re
from collections import defaultdict

# ================= é…ç½®è·¯å¾„ =================
# 1. ç¡®ä¿èƒ½å¯¼å…¥ç›¸å…³æ¨¡å—
sys.path.append("../Rec-Transformer") # LlamaRec æ‰€åœ¨
sys.path.append("../")


# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from CTR_models.src.DIN_evaluator import DINScorer
from llamarec import LlamaRecConfig, LlamaRecForCausalLM 
from sasrec import SasRecForCausalLM
from utils.RL_utils import GRPOTrainer_not_skip_special_token, RewardRunner
from utils.datacollator import EvalDataCollator, preprocess_function
from utils.utils_evaluate import DynamicHierarchicalLogitsProcessor, build_item_token_codebooks_dynamically
from utils.eval import compute_hr_at_k, compute_ndcg_at_k
from utils.utils import *

# ================= æ³¨å†Œæ¨¡å‹ =================
AutoConfig.register("llama-rec", LlamaRecConfig)
AutoModelForCausalLM.register(LlamaRecConfig, LlamaRecForCausalLM)

class GRPO_Eval_Trainer(GRPOTrainer_not_skip_special_token):
    def __init__(self, eval_dataset, generation_config_params, eval_collator, **kwargs):
        """
        ç»§æ‰¿è‡ª GRPOTrainerï¼Œä½†æ³¨å…¥äº†ä¸ Script B å®Œå…¨å¯¹é½çš„è¯„æµ‹é€»è¾‘
        """
        self.custom_eval_dataset = eval_dataset
        self.eval_collator = eval_collator  # æ¥æ”¶æ¥è‡ª Script B çš„ Collator
        
        # è§£åŒ…ç”Ÿæˆå‚æ•°
        self.gen_len = generation_config_params.get('generation_length', 4)
        self.num_beams = generation_config_params.get('num_beams', 1)
        self.k_values = generation_config_params.get('k_values', [1, 5, 10])
        self.item_token_codebooks = generation_config_params.get('item_token_codebooks', None)
        self.eval_sample_num = generation_config_params.get('eval_sample_num', 2000)

        # --- æ„å»º NumPy å‘é‡åŒ–æŸ¥æ‰¾è¡¨ ---
        print(">>> Building NumPy Vectorized Vocab Table for Evaluation...")
        vocab = kwargs['processing_class'].get_vocab()
        max_id = max(vocab.values())
        self.vocab_array = np.array(["" for _ in range(max_id + 1)], dtype=object)
        for k, v in vocab.items():
            self.vocab_array[v] = k
        print(">>> âœ… Vocab Table built.")

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        # æ³¨æ„ï¼šçˆ¶ç±»ä¸éœ€è¦ eval_datasetï¼Œå› ä¸ºæˆ‘ä»¬è¦åœ¨ evaluate ä¸­æ‰‹åŠ¨å¤„ç†å®ƒ
        # é¿å…çˆ¶ç±»å¯¹æˆ‘ä»¬çš„ eval_dataset åšä¸å¿…è¦çš„åˆ—ç§»é™¤æ“ä½œ
        super().__init__(eval_dataset=eval_dataset, **kwargs)

    # é‡å†™ evaluate æ–¹æ³•
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ datasetï¼Œå¦åˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶é¢„å¤„ç†å¥½çš„ dataset
        eval_ds = eval_dataset if eval_dataset is not None else self.custom_eval_dataset
        
        if eval_ds is None:
            print(">>> Warning: No eval dataset provided, skipping evaluation.")
            return {}
        
        if metric_key_prefix == "eval" and eval_ds is not None:
            total_size = len(eval_ds)
            if total_size > self.eval_sample_num:
                print(f"âš¡ [SpeedUp] Sampling {self.eval_sample_num} random examples from {total_size} for validation.")
                
                # éšæœºé€‰å–ç´¢å¼•
                # æ³¨æ„ï¼šè¿™é‡Œæ¯æ¬¡éªŒè¯éƒ½ä¼šé‡æ–°éšæœºï¼Œå¯¼è‡´éªŒè¯æŒ‡æ ‡ä¼šæœ‰æ³¢åŠ¨ï¼Œä½†èƒ½æ›´å…¨é¢åœ°ç›‘æ§æ¨¡å‹
                random_indices = random.sample(range(total_size), self.eval_sample_num)
                
                # ä½¿ç”¨ HuggingFace dataset çš„ select æ–¹æ³•åˆ›å»ºå­é›†
                eval_ds = eval_ds.select(random_indices)
            else:
                print(f"Dataset size ({total_size}) <= {self.eval_sample_num}, running full evaluation.")

        # 1. å‡†å¤‡ DataLoader
        # ç¡®ä¿æ•°æ®ç»è¿‡äº† preprocess_function å¤„ç†
        batch_size = self.args.per_device_eval_batch_size or self.args.per_device_train_batch_size
        eval_dataloader = DataLoader(
            eval_ds,
            batch_size=batch_size,
            collate_fn=self.eval_collator, # ä½¿ç”¨ä¼ å…¥çš„æ­£ç¡® Collator
            shuffle=False,
            drop_last=False
        )

        # åˆ‡æ¢æ¨¡å¼
        model = self.model
        model.eval()
        
        print(f"\n***** Running Generative Evaluation (Step {self.state.global_step}) *****")
        print(f"  Num examples = {len(eval_ds)}")
        print(f"  Batch size = {batch_size}")
        
        total_metrics_sum = {f"HR@{k}": 0.0 for k in self.k_values}
        total_metrics_sum.update({f"NDCG@{k}": 0.0 for k in self.k_values})
        total_samples = 0

        # 2. å¾ªç¯ç”Ÿæˆ
        with torch.no_grad():
            for batch in tqdm(eval_dataloader, desc="Evaluating"):
                input_ids = batch['input_ids'].to(self.args.device)
                attention_mask = batch['attention_mask'].to(self.args.device)
                groundtruth = batch['groundtruth'] # List[str]

                curr_bs = input_ids.shape[0]
                prompt_length = input_ids.shape[1]

                # æ„é€  Logits Processor (å¦‚æœæä¾›äº† codebooks)
                logits_processor = LogitsProcessorList()
                if self.item_token_codebooks:
                    logits_processor.append(
                        DynamicHierarchicalLogitsProcessor(
                            prompt_length=prompt_length,
                            item_token_codebooks=self.item_token_codebooks,
                            device=self.args.device
                        )
                    )

                # 3. Beam Search ç”Ÿæˆ (HuggingFace Generate)
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=prompt_length + self.gen_len,
                    num_beams=self.num_beams,
                    do_sample=False, 
                    num_return_sequences=self.num_beams,
                    pad_token_id=self.processing_class.pad_token_id,
                    eos_token_id=self.processing_class.eos_token_id,
                    logits_processor=logits_processor,
                    use_cache=True
                )

                # 4. ã€æé€Ÿè§£ç ã€‘(NumPy Vectorized)
                # shape: [Batch_Size * Num_Beams, Gen_Len]
                new_tokens_cpu = generated_ids[:, -self.gen_len:].cpu().numpy()
                token_strs = self.vocab_array[new_tokens_cpu] # O(1) æŸ¥è¡¨
                
                # å‘é‡åŒ–å­—ç¬¦ä¸²æ‹¼æ¥
                if self.gen_len == 1:
                    predicted_token_sequences = token_strs.flatten().tolist()
                else:
                    result_array = token_strs[:, 0]
                    for i in range(1, self.gen_len):
                        result_array = result_array + token_strs[:, i]
                    predicted_token_sequences = result_array.tolist()

                # 5. Reshape ä¸º [Batch, Num_Beams]
                reshaped_token_sequences = [
                    predicted_token_sequences[i : i + self.num_beams]
                    for i in range(0, len(predicted_token_sequences), self.num_beams)
                ]

                # 6. è®¡ç®—æŒ‡æ ‡
                batch_hr = compute_hr_at_k(reshaped_token_sequences, groundtruth, self.k_values)
                batch_ndcg = compute_ndcg_at_k(reshaped_token_sequences, groundtruth, self.k_values)

                for k_val in self.k_values:
                    total_metrics_sum[f"HR@{k_val}"] += batch_hr[f"HR@{k_val}"] * curr_bs
                    total_metrics_sum[f"NDCG@{k_val}"] += batch_ndcg[f"NDCG@{k_val}"] * curr_bs
                
                total_samples += curr_bs
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        model.train()

        # 7. æ±‡æ€»å¹¶ Log
        metrics = {f"{metric_key_prefix}_{k}": (v / total_samples) for k, v in total_metrics_sum.items()}
        
        # è°ƒç”¨ Trainer å†…ç½®çš„ log æ–¹æ³•ï¼Œè¿™æ · Wandb å’Œæ—¥å¿—æ–‡ä»¶éƒ½èƒ½è®°å½•åˆ°
        self.log(metrics)
        
        # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼šå°† metric ä¼ å›ç»™ Trainer çš„ control ç³»ç»Ÿï¼Œç”¨äºæ—©åœåˆ¤æ–­
        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
        
        # ä»æ‰“å°ç»“æœæ¥çœ‹ï¼Œå¥½åƒtransformersä¼šè‡ªåŠ¨æ‰“å°metricï¼Œå’Œä¸‹é¢çš„å‡ ä¹åˆ«æ— äºŒè‡´
        # print(f">>> Evaluation Metrics: {metrics}")
        return metrics


class RewardRunner_prefix:
    # ================= å†…éƒ¨ç±»å®šä¹‰ =================
    class TrieNode:
        def __init__(self):
            # ä½¿ç”¨å­—å…¸å­˜å‚¨å­èŠ‚ç‚¹ï¼Œkeyä¸ºtokenå­—ç¬¦ä¸²
            self.children = {}
            # ç»è¿‡è¯¥èŠ‚ç‚¹çš„è·¯å¾„æ•°é‡ï¼ˆæµè¡Œåº¦ï¼‰
            self.count = 0
            
    # ================= ä¸»ç±»é€»è¾‘ =================
    def __init__(self, scorer=None, weight=0.8, trie_weight=1.0, penalty=-1.0, name="reward_combined"):
        """
        :param scorer: DINScorer å®ä¾‹ (å¯é€‰)
        :param weight: DIN åˆ†æ•°æƒé‡
        :param trie_weight: å‰ç¼€æ ‘ Token-level å¥–åŠ±çš„æƒé‡
        :param penalty: æ ¼å¼é”™è¯¯æƒ©ç½š
        """
        self.scorer = scorer
        self.weight = weight
        self.trie_weight = trie_weight
        self.penalty = penalty
        self.__name__ = name

    def _parse_items(self, text):
        """æ­£åˆ™æå– <item_id>"""
        return re.findall(r"<[^>]+>", text)

    def _build_batch_trie(self, ground_truths):
        """
        ä¸ºå•ä¸ªæ ·æœ¬æ„å»º Trie æ ‘ã€‚
        :param ground_truths: List[str] æˆ– strã€‚å½“å‰ User çš„æ‰€æœ‰æ­£ç¡®ç­”æ¡ˆè·¯å¾„ã€‚
        """
        if isinstance(ground_truths, str):
            ground_truths = [ground_truths]
            
        root = self.TrieNode()
        # æ ¹èŠ‚ç‚¹çš„ count ç­‰äºè¯¥æ ·æœ¬æ‰€æœ‰ GT çš„æ€»æ•°
        root.count = len(ground_truths)
        
        for gt_str in ground_truths:
            tokens = self._parse_items(gt_str)
            node = root
            for token in tokens:
                if token not in node.children:
                    node.children[token] = self.TrieNode()
                node = node.children[token]
                # è·¯å¾„ç»è¿‡æ­¤èŠ‚ç‚¹ï¼Œè®¡æ•°+1
                node.count += 1
        return root

    def _compute_token_level_trie_score(self, completion, ground_truth):
        """
        è®¡ç®—ç´¯åŠ çš„ Token-level æ¦‚ç‡å¥–åŠ±ã€‚
        """
        # 1. æ ¼å¼æ ¡éªŒ
        c_stripped = completion.strip()
        if not c_stripped.startswith("<") or not c_stripped.endswith(">"):
            return self.penalty

        # 2. è§£æç”Ÿæˆçš„åºåˆ—
        gen_tokens = self._parse_items(c_stripped)
        if not gen_tokens:
            return self.penalty

        # 3. æ„å»º Trie (é’ˆå¯¹å½“å‰æ ·æœ¬çš„ GT é›†åˆ)
        root = self._build_batch_trie(ground_truth)

        # 4. é€ Token åŒ¹é…å¹¶è®¡ç®—æ¦‚ç‡
        current_node = root
        accumulated_prob_score = 0.0
        
        for token in gen_tokens:
            if token in current_node.children:
                next_node = current_node.children[token]
                
                # === æ ¸å¿ƒç®—æ³•ï¼šToken ç²’åº¦çš„æ¦‚ç‡ ===
                # çˆ¶èŠ‚ç‚¹æœ‰ N æ¡è·¯ï¼Œå…¶ä¸­ M æ¡è·¯èµ°äº†å½“å‰ token
                # Reward_t = M / N
                # è¿™æ„å‘³ç€æ¨¡å‹èµ°äº†ä¸€æ¡â€œå¤§è·¯â€ï¼ˆé«˜æµè¡Œåº¦è·¯å¾„ï¼‰ä¼šå¾—é«˜åˆ†ï¼Œèµ°â€œå°è·¯â€å¾—ä½åˆ†
                step_reward = next_node.count / current_node.count
                
                accumulated_prob_score += step_reward
                
                # æŒ‡é’ˆä¸‹ç§»
                current_node = next_node
            else:
                # åŒ¹é…ä¸­æ–­ï¼šåç»­ Token æ— æ³•åœ¨ GT æ ‘ä¸­æ‰¾åˆ°ï¼Œåœæ­¢å¥–åŠ±
                # è¿™é‡Œå¯ä»¥é€‰æ‹©ç»™ä¸€ä¸ªå°çš„ step penaltyï¼Œæˆ–è€…ç›´æ¥ break
                break
        
        if len(gen_tokens) > 0:
            return accumulated_prob_score / len(gen_tokens)
        else:
            return 0.0

    def __call__(self, prompts, completions, ground_truth, user_id, **kwargs):
        """
        TRL å›è°ƒå…¥å£
        """
        # è¿™é‡Œçš„ prompts å…¶å®å°±æ˜¯ historyï¼Œæ ¹æ®ä½ çš„ä»£ç é€»è¾‘
        history = prompts
        
        # ç»“æœåˆ—è¡¨
        final_rewards = [0.0] * len(completions)
        
        # æ”¶é›†éœ€è¦ DIN æ‰“åˆ†çš„æ ·æœ¬
        din_batch_indices = []
        din_batch_uids = []
        din_batch_hist = []
        din_batch_comp = []

        for i, (c, gt) in enumerate(zip(completions, ground_truth)):
            
            if c in gt:
                # ç›´æ¥å‘½ä¸­ GTï¼Œç»™æœ€é«˜åˆ†
                final_rewards[i] = 1.0
                continue
            # --- éƒ¨åˆ† 1: å‰ç¼€æ ‘æ¦‚ç‡å¥–åŠ± (Token-level Accumulation) ---
            # è¿™æ˜¯ä¸€ä¸ªå¯†é›†å¥–åŠ± (Dense Reward)
            trie_score = self._compute_token_level_trie_score(c, gt)
            
            # å¦‚æœæ ¼å¼é”™è¯¯ï¼Œç›´æ¥æƒ©ç½šå¹¶è·³è¿‡ DIN
            if trie_score == self.penalty:
                final_rewards[i] = self.penalty
                continue
            
            final_rewards[i] = trie_score * self.trie_weight

            # --- éƒ¨åˆ† 2: å‡†å¤‡ DIN æ‰“åˆ† (Sequence-level Reward) ---
            # åªæœ‰å½“æ¨¡å‹é…ç½®äº† scorer ä¸”æƒé‡ä¸ä¸º0æ—¶æ‰è®¡ç®—
            if self.scorer and self.weight > 0:
                din_batch_indices.append(i)
                din_batch_uids.append(user_id[i])
                din_batch_hist.append(history[i])
                din_batch_comp.append(c)

        # --- éƒ¨åˆ† 3: æ‰¹é‡æ‰§è¡Œ DIN ---
        if din_batch_indices:
            try:
                # å‡è®¾ scorer.predict_batch è¿”å› list of floats
                din_scores = self.scorer.predict_batch(
                    user_ids=din_batch_uids,
                    history=din_batch_hist,
                    completions=din_batch_comp
                )
                
                for idx, d_score in zip(din_batch_indices, din_scores):
                    # å åŠ å¥–åŠ±ï¼š Tokenç´¯åŠ åˆ† + DINæ•´å¥åˆ†
                    final_rewards[idx] += max(-0.1, d_score) * self.weight
                    
            except Exception as e:
                # å®¹é”™å¤„ç†ï¼Œé¿å…è®­ç»ƒä¸­æ–­ï¼Œä»…æ‰“å°é”™è¯¯
                print(f"[RewardRunner Error] DIN inference failed: {e}")

        return final_rewards

if __name__ == '__main__':
    # ================= å‘½ä»¤è¡Œå‚æ•°è§£æ =================
    parser = argparse.ArgumentParser(description="GRPO Training with YAML Config")
    parser.add_argument("--config", type=str, default="./rl_configs/KuaiRec_big_llamarec_DIN.yaml", help="Path to the YAML config file")
    args_cli = parser.parse_args()

    # ================= åŠ è½½ config =================
    print(f">>> Loading configuration from {args_cli.config}...")
    with open(args_cli.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # æå–å„ä¸ªéƒ¨åˆ†çš„é…ç½®
    paths_cfg = config['paths']
    din_cfg = config['din']
    train_cfg = config['training']
    eval_cfg = config['evaluation']

    # ================= åŠ è½½ DIN Scorer =================
    print(">>> Initializing DIN Reward Model...")
    
    din_scorer = DINScorer(
        config_dir=paths_cfg['din_config_dir'],
        model_dir=paths_cfg['din_model_dir'],
        experiment_id=din_cfg['experiment_id'],
        data_dir=paths_cfg['din_data_dir'],
        device=din_cfg['device']                   
    )
    print(">>> DIN Scorer Ready.")

    reward_runner = RewardRunner_prefix(
        scorer=din_scorer, 
        weight=din_cfg['reward_weight'],
        penalty=din_cfg.get('penalty', -1.0)
    )

    # ================= åŠ è½½æ¨¡å‹ä¸Tokenizer =================
    print(">>> Loading LLM...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    llm_path = paths_cfg['llm_model_path']
    # åŠ¨æ€é€‰æ‹©æ¨¡å‹ç±»
    if train_cfg['model_name'] == 'sasrec':
        model_class = SasRecForCausalLM
    elif train_cfg['model_name'] == 'llama':
        model_class = LlamaForCausalLM
    elif train_cfg['model_name'].startswith('qwen'):
        model_class = Qwen2ForCausalLM
    else:
        model_class = LlamaRecForCausalLM

    try:
        model = model_class.from_pretrained(
            llm_path, 
            torch_dtype=torch.bfloat16 if train_cfg.get('bf16', False) else "auto",
            device_map=device
        )
    except Exception as e:
        print(f"Failed to load model from {llm_path}. Error: {e}")
        # å°è¯•è‡ªåŠ¨å›é€€
        print("Trying AutoModelForCausalLM...")
        model = AutoModelForCausalLM.from_pretrained(llm_path, device_map=device)
    tokenizer = AutoTokenizer.from_pretrained(llm_path, trust_remote_code=True)

    # Tokenizer è¡¥ä¸
    if tokenizer.model_input_names is not None and "token_type_ids" in tokenizer.model_input_names:
        tokenizer.model_input_names.remove("token_type_ids")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'
    tokenizer.truncation_side = 'left'

    # ================= å‡†å¤‡è¯„ä¼°ç”¨çš„ Codebook =================
    generation_length = eval_cfg['generation_length']
    item_token_codebooks = build_item_token_codebooks_dynamically(tokenizer, generation_length)

    # ================= æ•°æ®é›†åŠ è½½ =================
    rl_data_dir = paths_cfg['rl_data_dir']
    
    # 1. åŠ è½½ Train
    train_json_path = os.path.join(rl_data_dir, paths_cfg['train_file'])
    print(f">>> Loading Train Dataset: {train_json_path}")
    train_dataset = load_dataset("json", data_files=train_json_path, split="train")

    # 2. åŠ è½½ Test
    test_json_path = os.path.join(rl_data_dir, paths_cfg['test_file'])
    print(f">>> Loading Test Dataset: {test_json_path}")
    raw_test_dataset = load_dataset("json", data_files=test_json_path, split="train")

    # ================= è®­ç»ƒè¾“å‡ºè·¯å¾„é…ç½® =================
    base_dir = paths_cfg['output_root']
    date_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(base_dir, f"checkpoints_{date_time}")
    os.makedirs(output_dir, exist_ok=True)
    print(f">>> Checkpoints will be saved to: {output_dir}")

    max_seq_length = train_cfg['max_seq_length']
    print(f">>> Preprocessing Test Dataset... Max Length: {max_seq_length}")
    # ä½¿ç”¨ Script B åŒæ¬¾çš„ preprocess_function
    test_dataset = raw_test_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=8, 
        remove_columns=['prompt'], # è¿™é‡Œè¦æ³¨æ„ï¼špreprocess_function ä¼šç”Ÿæˆ input_idsï¼Œæˆ‘ä»¬åªä¿ç•™éœ€è¦çš„åˆ—
        fn_kwargs={"tokenizer": tokenizer, "max_seq_length": max_seq_length},
        desc="Tokenizing Test Set"
    )
    eval_collator = EvalDataCollator(tokenizer=tokenizer, max_length=max_seq_length)

    # é€»è¾‘ï¼šæ€»æ—¥å¿—ç›®å½• / æœ¬æ¬¡å®éªŒåç§°(å¸¦æ—¶é—´æˆ³)
    # è¿™æ ·ä½ åœ¨ tensorboard --logdir ./temp_try_GRPO_Rec_Output/all_tensorboard_logs æ—¶èƒ½çœ‹åˆ°æ‰€æœ‰å®éªŒçš„æ›²çº¿å¯¹æ¯”
    tb_root = paths_cfg.get('tensorboard_root', './temp_try_GRPO_Rec_Output/all_tensorboard_logs')
    tb_dir = os.path.join(tb_root, f"run_{date_time}")
    
    print(f">>> TensorBoard logs will be saved to: {tb_dir}")

    # ================= è®­ç»ƒå‚æ•°é…ç½® =================
    # ä» yaml ä¸­æå– evaluate/save çš„æ­¥æ•°
    eval_save_steps = train_cfg['eval_save_steps']

    training_args = GRPOConfig(
        output_dir=output_dir,
        logging_dir = tb_dir,
        report_to="tensorboard",
        
        learning_rate=float(train_cfg['learning_rate']), # ç¡®ä¿ YAML è¯»å–çš„æ˜¯ float
        num_train_epochs=train_cfg['num_train_epochs'],
        per_device_train_batch_size=train_cfg['per_device_train_batch_size'],
        gradient_accumulation_steps=train_cfg['gradient_accumulation_steps'],
        
        # æ—¥å¿—
        logging_steps=train_cfg['logging_steps'],
        
        # ç”Ÿæˆå‚æ•° (RL Training)
        max_completion_length=train_cfg['max_completion_length'],
        num_generations=train_cfg['num_generations'],
        use_vllm=train_cfg['use_vllm'],
        bf16=train_cfg['bf16'] if 'bf16' in train_cfg else False,
        mask_truncated_completions=train_cfg.get('mask_truncated_completions', False),
        temperature=0.7,        # ç¨å¾®é™ä¸€ç‚¹ï¼Œé¿å…ç”Ÿæˆå®Œå…¨ä¹±ç çš„ Item ID
        top_k=50,               # é™åˆ¶é‡‡æ ·èŒƒå›´ï¼Œé¿å…é‡‡æ ·åˆ°æå…¶å†·é—¨çš„ Item
        top_p=0.95,

        # è¯„ä¼°ä¸æ—©åœç­–ç•¥
        eval_strategy="steps",
        eval_steps=eval_save_steps,
        per_device_eval_batch_size=train_cfg['per_device_eval_batch_size'],

        save_strategy="steps",
        save_steps=eval_save_steps,
        save_total_limit=train_cfg['save_total_limit'],
        
        load_best_model_at_end=True,
        metric_for_best_model=train_cfg['metric_for_best_model'],
        greater_is_better=True,
        
        # é˜²æ­¢åˆ æ‰ prompt/ground_truth åˆ—
        remove_unused_columns=False
    )
    
    # ç»„è£…è¯„ä¼°é…ç½®å­—å…¸
    eval_config_dict = {
        "generation_length": generation_length,
        "num_beams": eval_cfg['num_beams'],
        "k_values": eval_cfg['k_values'],
        "item_token_codebooks": item_token_codebooks,
        "eval_sample_num": eval_cfg.get('eval_sample_num', 2000)
    }

    # ================= åˆå§‹åŒ– Trainer =================
    trainer = GRPO_Eval_Trainer(
        model=model,
        reward_funcs=[reward_runner],
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        eval_collator=eval_collator,
        generation_config_params=eval_config_dict,
        processing_class=tokenizer,
        
        # æ—©åœå›è°ƒ
        callbacks=[EarlyStoppingCallback(
            early_stopping_patience=train_cfg['early_stopping_patience']
        )]
    )

    # ================= å¼€å§‹è®­ç»ƒ =================
    print(">>> Starting GRPO Training with Live Evaluation...")
    trainer.train()

    # ================= æ‰“å°æœ€ä½³æ¨¡å‹ç»“æœ =================
    # è·å–æœ€ä½³ Checkpoint çš„è·¯å¾„
    best_ckpt_path = trainer.state.best_model_checkpoint
    
    if best_ckpt_path:
        print(f"\n" + "="*50)
        print(f"ğŸ† TRAINING FINISHED. BEST MODEL FOUND.")
        print(f"="*50)
        print(f"ğŸ“ Best Checkpoint Path: {best_ckpt_path}")
        print(f"ğŸŒŸ Best Metric Value:    {trainer.state.best_metric}")
        
        # --- æ ¸å¿ƒé€»è¾‘ï¼šä»æ—¥å¿—å†å²ä¸­æå‡ºæœ€ä½³é‚£ä¸€æ­¥çš„å®Œæ•´æŒ‡æ ‡ ---
        # 1. ä»è·¯å¾„ä¸­æå–æœ€ä½³æ­¥æ•° (ä¾‹å¦‚ "xxx/checkpoint-500" -> 500)
        try:
            best_step = int(best_ckpt_path.split('-')[-1])
            
            # 2. éå†æ—¥å¿—å†å²æ‰¾åˆ°é‚£ä¸€åˆ»çš„è¯¦ç»†æ•°æ®
            best_log_entry = None
            for log in trainer.state.log_history:
                # å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šæ˜¯è¿™ä¸€æ­¥ï¼Œä¸”åŒ…å«è¯„ä¼°æŒ‡æ ‡(æ¯”å¦‚æœ‰ eval_loss æˆ– eval_NDCG@10)
                if log.get("step") == best_step and "eval_NDCG@10" in log:
                    best_log_entry = log
                    break
            
            if best_log_entry:
                print(f"\nğŸ“Š Detailed Metrics for Best Model (Step {best_step}):")
                # æ ¼å¼åŒ–æ‰“å°å­—å…¸
                for k, v in best_log_entry.items():
                    if k.startswith("eval_"):
                        print(f"   - {k}: {v}")
            else:
                print(f"âš ï¸ Could not find detailed logs for step {best_step} in history.")

        except Exception as e:
            print(f"âš ï¸ Error parsing best step info: {e}")

    # ================= ä¿å­˜æœ€ç»ˆæ¨¡å‹ =================
    final_save_path = os.path.join(output_dir, "final_best_grpo_model")
    trainer.save_model(final_save_path)
    print(f">>> Training Finished & Best Model Saved to {final_save_path}")