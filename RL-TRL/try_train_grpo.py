# train_grpo.py
import sys
import os
import yaml
import argparse
from datetime import datetime
import torch
from datasets import load_dataset
from tqdm import tqdm
from trl import GRPOConfig
from transformers import (
    AutoConfig, 
    AutoModelForCausalLM, 
    AutoTokenizer, 
    LogitsProcessorList, 
    TrainerCallback, 
    EarlyStoppingCallback,
)
from torch.utils.data import DataLoader
import numpy as np

# ================= é…ç½®è·¯å¾„ =================
# 1. ç¡®ä¿èƒ½å¯¼å…¥ç›¸å…³æ¨¡å—
sys.path.append("../Rec-Transformer") # LlamaRec æ‰€åœ¨
sys.path.append("../")

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from CTR_models.DIN.DIN_evaluator import DINScorer
from llamarec import LlamaRecConfig, LlamaRecForCausalLM 
from utils.RL_utils import GRPOTrainer_not_skip_special_token, DINRewardRunner
from utils.utils_evaluate import DynamicHierarchicalLogitsProcessor, build_item_token_codebooks_dynamically
from utils.eval import compute_hr_at_k, compute_ndcg_at_k

# ================= æ³¨å†Œæ¨¡åž‹ =================
AutoConfig.register("llama-rec", LlamaRecConfig)
AutoModelForCausalLM.register(LlamaRecConfig, LlamaRecForCausalLM)


class GRPO_Eval_Trainer(GRPOTrainer_not_skip_special_token):
    def __init__(self, eval_dataset, generation_config_params, **kwargs):
        """
        ç»§æ‰¿è‡ª GRPOTrainerï¼Œä½†æ³¨å…¥äº† CustomTrainer çš„è¯„æµ‹é€»è¾‘
        """
        self.custom_eval_dataset = eval_dataset
        # è§£åŒ…ç”Ÿæˆå‚æ•°
        self.gen_len = generation_config_params.get('generation_length', 4)
        self.num_beams = generation_config_params.get('num_beams', 1)
        self.k_values = generation_config_params.get('k_values', [1, 5, 10])
        self.item_token_codebooks = generation_config_params.get('item_token_codebooks', None)

        # --- ã€æ ¸å¿ƒç§»æ¤ã€‘æž„å»º NumPy å‘é‡åŒ–æŸ¥æ‰¾è¡¨ ---
        # è¿™éƒ¨åˆ†é€»è¾‘ç›´æŽ¥æ¥è‡ªä½ çš„ CustomTrainer
        print(">>> Building NumPy Vectorized Vocab Table for Evaluation...")
        vocab = kwargs['processing_class'].get_vocab()
        max_id = max(vocab.values())
        # åˆå§‹åŒ– object æ•°ç»„
        self.vocab_array = np.array(["" for _ in range(max_id + 1)], dtype=object)
        for k, v in vocab.items():
            self.vocab_array[v] = k
        print(">>> âœ… Vocab Table built.")

        # 3. ã€å…³é”®ä¿®æ”¹ã€‘è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ—¶ï¼Œæ˜¾å¼ä¼ å…¥ eval_dataset
        super().__init__(eval_dataset=eval_dataset, **kwargs)

    # ç®€å•çš„ Collatorï¼šä¸“é—¨ç”¨äºŽè¯„ä¼°æ—¶çš„ tokenization
    # å› ä¸º RL Dataset æ˜¯ {'prompt': str, 'ground_truth': str}ï¼Œéœ€è¦è½¬ Tensor
    def _eval_collator(self, batch):
        prompts = [x['prompt'] for x in batch]
        ground_truths = [x['ground_truth'] for x in batch]
        
        # å®žæ—¶ Tokenize
        inputs = self.processing_class(
            prompts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            padding_side='left',
            max_length=200,
        )
        
        return {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "groundtruth": ground_truths
        }

    # é‡å†™ evaluate æ–¹æ³•
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        # ä½¿ç”¨æˆ‘ä»¬ä¼ å…¥çš„ dataset
        eval_ds = eval_dataset if eval_dataset is not None else self.custom_eval_dataset
        
        if eval_ds is None:
            print(">>> Warning: No eval dataset provided, skipping evaluation.")
            return {}

        # 1. å‡†å¤‡ DataLoader
        # ä½¿ç”¨ args.per_device_eval_batch_size
        batch_size = self.args.per_device_eval_batch_size or self.args.per_device_train_batch_size
        eval_dataloader = DataLoader(
            eval_ds,
            batch_size=batch_size,
            collate_fn=self._eval_collator,
            shuffle=False,
            drop_last=False
        )

        # åˆ‡æ¢æ¨¡å¼
        model = self.model
        model.eval()
        
        print(f"\n***** Running Generative Evaluation (Step {self.state.global_step}) *****")
        
        total_metrics_sum = {f"HR@{k}": 0.0 for k in self.k_values}
        total_metrics_sum.update({f"NDCG@{k}": 0.0 for k in self.k_values})
        total_samples = 0
        
        # 2. å¾ªçŽ¯ç”Ÿæˆ
        with torch.no_grad():
            for batch in tqdm(eval_dataloader, desc="Evaluating"):
                input_ids = batch['input_ids'].to(self.args.device)
                attention_mask = batch['attention_mask'].to(self.args.device)
                groundtruth = batch['groundtruth'] # List[str]

                curr_bs = input_ids.shape[0]
                prompt_length = input_ids.shape[1]

                # æž„é€  Logits Processor (å¦‚æžœæä¾›äº† codebooks)
                logits_processor = LogitsProcessorList()
                if self.item_token_codebooks:
                    logits_processor.append(
                        DynamicHierarchicalLogitsProcessor(
                            prompt_length=prompt_length,
                            item_token_codebooks=self.item_token_codebooks,
                            device=self.args.device
                        )
                    )

                # 3. Beam Search ç”Ÿæˆ (HuggingFace Generate)
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=prompt_length + self.gen_len,
                    num_beams=self.num_beams,
                    do_sample=False, 
                    num_return_sequences=self.num_beams,
                    pad_token_id=self.processing_class.pad_token_id,
                    eos_token_id=self.processing_class.eos_token_id,
                    logits_processor=logits_processor,
                    use_cache=True
                )

                # 4. ã€æžé€Ÿè§£ç ã€‘(NumPy Vectorized)
                # shape: [Batch_Size * Num_Beams, Gen_Len]
                new_tokens_cpu = generated_ids[:, -self.gen_len:].cpu().numpy()
                token_strs = self.vocab_array[new_tokens_cpu] # O(1) æŸ¥è¡¨
                
                # å‘é‡åŒ–å­—ç¬¦ä¸²æ‹¼æŽ¥
                if self.gen_len == 1:
                    predicted_token_sequences = token_strs.flatten().tolist()
                else:
                    result_array = token_strs[:, 0]
                    for i in range(1, self.gen_len):
                        result_array = result_array + token_strs[:, i]
                    predicted_token_sequences = result_array.tolist()

                # 5. Reshape ä¸º [Batch, Num_Beams]
                reshaped_token_sequences = [
                    predicted_token_sequences[i : i + self.num_beams]
                    for i in range(0, len(predicted_token_sequences), self.num_beams)
                ]

                # 6. è®¡ç®—æŒ‡æ ‡
                batch_hr = compute_hr_at_k(reshaped_token_sequences, groundtruth, self.k_values)
                batch_ndcg = compute_ndcg_at_k(reshaped_token_sequences, groundtruth, self.k_values)

                for k_val in self.k_values:
                    total_metrics_sum[f"HR@{k_val}"] += batch_hr[f"HR@{k_val}"] * curr_bs
                    total_metrics_sum[f"NDCG@{k_val}"] += batch_ndcg[f"NDCG@{k_val}"] * curr_bs
                
                total_samples += curr_bs
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        model.train()

        # 7. æ±‡æ€»å¹¶ Log
        metrics = {f"{metric_key_prefix}_{k}": (v / total_samples) for k, v in total_metrics_sum.items()}
        
        # è°ƒç”¨ Trainer å†…ç½®çš„ log æ–¹æ³•ï¼Œè¿™æ · Wandb å’Œæ—¥å¿—æ–‡ä»¶éƒ½èƒ½è®°å½•åˆ°
        self.log(metrics)
        
        # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼šå°† metric ä¼ å›žç»™ Trainer çš„ control ç³»ç»Ÿï¼Œç”¨äºŽæ—©åœåˆ¤æ–­
        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
        
        print(f">>> Evaluation Metrics: {metrics}")
        return metrics


if __name__ == '__main__':
    # ================= å‘½ä»¤è¡Œå‚æ•°è§£æž =================
    parser = argparse.ArgumentParser(description="GRPO Training with YAML Config")
    parser.add_argument("--config", type=str, default="grpo_config.yaml", help="Path to the YAML config file")
    args_cli = parser.parse_args()

    # ================= åŠ è½½ config =================
    print(f">>> Loading configuration from {args_cli.config}...")
    with open(args_cli.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # æå–å„ä¸ªéƒ¨åˆ†çš„é…ç½®
    paths_cfg = config['paths']
    din_cfg = config['din']
    train_cfg = config['training']
    eval_cfg = config['evaluation']

    # ================= åŠ è½½ DIN Scorer =================
    print(">>> Initializing DIN Reward Model...")
    
    din_scorer = DINScorer(
        config_dir=paths_cfg['din_config_dir'],
        model_dir=paths_cfg['din_model_dir'],
        experiment_id=din_cfg['experiment_id'],
        data_dir=paths_cfg['din_data_dir'],
        device=din_cfg['device']                   
    )
    print(">>> DIN Scorer Ready.")

    reward_runner = DINRewardRunner(
        scorer=din_scorer, 
        weight=din_cfg['reward_weight'],
        penalty=din_cfg.get('penalty', -1.0)
    )

    # ================= æ•°æ®é›†åŠ è½½ =================
    rl_data_dir = paths_cfg['rl_data_dir']
    
    # 1. åŠ è½½ Train
    train_json_path = os.path.join(rl_data_dir, paths_cfg['train_file'])
    print(f">>> Loading Train Dataset: {train_json_path}")
    train_dataset = load_dataset("json", data_files=train_json_path, split='train')

    # 2. åŠ è½½ Valid
    test_json_path = os.path.join(rl_data_dir, paths_cfg['test_file'])
    print(f">>> Loading Test Dataset: {test_json_path}")
    test_dataset = load_dataset("json", data_files=test_json_path, split='train')

    # ================= è®­ç»ƒè¾“å‡ºè·¯å¾„é…ç½® =================
    base_dir = paths_cfg['output_root']
    date_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(base_dir, f"checkpoints_{date_time}")
    os.makedirs(output_dir, exist_ok=True)
    print(f">>> Checkpoints will be saved to: {output_dir}")

    # é€»è¾‘ï¼šæ€»æ—¥å¿—ç›®å½• / æœ¬æ¬¡å®žéªŒåç§°(å¸¦æ—¶é—´æˆ³)
    # è¿™æ ·ä½ åœ¨ tensorboard --logdir ./temp_try_GRPO_Rec_Output/all_tensorboard_logs æ—¶èƒ½çœ‹åˆ°æ‰€æœ‰å®žéªŒçš„æ›²çº¿å¯¹æ¯”
    tb_root = paths_cfg.get('tensorboard_root', './temp_try_GRPO_Rec_Output/all_tensorboard_logs')
    tb_dir = os.path.join(tb_root, f"run_{date_time}")
    
    print(f">>> TensorBoard logs will be saved to: {tb_dir}")

    # ================= è®­ç»ƒå‚æ•°é…ç½® =================
    # ä»Ž yaml ä¸­æå– evaluate/save çš„æ­¥æ•°
    eval_save_steps = train_cfg['eval_save_steps']

    training_args = GRPOConfig(
        output_dir=output_dir,
        logging_dir = tb_dir,
        report_to="tensorboard",
        
        learning_rate=float(train_cfg['learning_rate']), # ç¡®ä¿ YAML è¯»å–çš„æ˜¯ float
        num_train_epochs=train_cfg['num_train_epochs'],
        per_device_train_batch_size=train_cfg['per_device_train_batch_size'],
        gradient_accumulation_steps=train_cfg['gradient_accumulation_steps'],
        
        # æ—¥å¿—
        logging_steps=train_cfg['logging_steps'],
        
        # ç”Ÿæˆå‚æ•° (RL Training)
        max_completion_length=train_cfg['max_completion_length'],
        num_generations=train_cfg['num_generations'],
        use_vllm=train_cfg['use_vllm'],
        bf16=train_cfg['bf16'],

        # è¯„ä¼°ä¸Žæ—©åœç­–ç•¥
        eval_strategy="steps",
        eval_steps=eval_save_steps,
        per_device_eval_batch_size=train_cfg['per_device_train_batch_size'], # é»˜è®¤å’Œ train ä¸€æ ·

        save_strategy="steps",
        save_steps=eval_save_steps,
        save_total_limit=train_cfg['save_total_limit'],
        
        load_best_model_at_end=True,
        metric_for_best_model=train_cfg['metric_for_best_model'],
        greater_is_better=True,
        
        # é˜²æ­¢åˆ æŽ‰ prompt/ground_truth åˆ—
        remove_unused_columns=False
    )

    # ================= åŠ è½½æ¨¡åž‹ä¸ŽTokenizer =================
    print(">>> Loading LLM...")
    llm_path = paths_cfg['llm_model_path']
    model = LlamaRecForCausalLM.from_pretrained(llm_path)
    tokenizer = AutoTokenizer.from_pretrained(llm_path)

    # Tokenizer è¡¥ä¸
    if tokenizer.model_input_names is not None and "token_type_ids" in tokenizer.model_input_names:
        tokenizer.model_input_names.remove("token_type_ids")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ================= å‡†å¤‡è¯„ä¼°ç”¨çš„ Codebook =================
    generation_length = eval_cfg['generation_length']
    item_token_codebooks = build_item_token_codebooks_dynamically(tokenizer, generation_length)
    
    # ç»„è£…è¯„ä¼°é…ç½®å­—å…¸
    eval_config_dict = {
        "generation_length": generation_length,
        "num_beams": eval_cfg['num_beams'],
        "k_values": eval_cfg['k_values'],
        "item_token_codebooks": item_token_codebooks
    }

    # ================= åˆå§‹åŒ– Trainer =================
    trainer = GRPO_Eval_Trainer(
        model=model,
        reward_funcs=[reward_runner],
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        generation_config_params=eval_config_dict,
        processing_class=tokenizer,
        
        # æ—©åœå›žè°ƒ
        callbacks=[EarlyStoppingCallback(
            early_stopping_patience=train_cfg['early_stopping_patience']
        )]
    )

    # ================= å¼€å§‹è®­ç»ƒ =================
    print(">>> Starting GRPO Training with Live Evaluation...")
    trainer.train()

    # ================= æ‰“å°æœ€ä½³æ¨¡åž‹ç»“æžœ =================
    # èŽ·å–æœ€ä½³ Checkpoint çš„è·¯å¾„
    best_ckpt_path = trainer.state.best_model_checkpoint
    
    if best_ckpt_path:
        print(f"\n" + "="*50)
        print(f"ðŸ† TRAINING FINISHED. BEST MODEL FOUND.")
        print(f"="*50)
        print(f"ðŸ“ Best Checkpoint Path: {best_ckpt_path}")
        print(f"ðŸŒŸ Best Metric Value:    {trainer.state.best_metric}")
        
        # --- æ ¸å¿ƒé€»è¾‘ï¼šä»Žæ—¥å¿—åŽ†å²ä¸­æžå‡ºæœ€ä½³é‚£ä¸€æ­¥çš„å®Œæ•´æŒ‡æ ‡ ---
        # 1. ä»Žè·¯å¾„ä¸­æå–æœ€ä½³æ­¥æ•° (ä¾‹å¦‚ "xxx/checkpoint-500" -> 500)
        try:
            best_step = int(best_ckpt_path.split('-')[-1])
            
            # 2. éåŽ†æ—¥å¿—åŽ†å²æ‰¾åˆ°é‚£ä¸€åˆ»çš„è¯¦ç»†æ•°æ®
            best_log_entry = None
            for log in trainer.state.log_history:
                # å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šæ˜¯è¿™ä¸€æ­¥ï¼Œä¸”åŒ…å«è¯„ä¼°æŒ‡æ ‡(æ¯”å¦‚æœ‰ eval_loss æˆ– eval_NDCG@10)
                if log.get("step") == best_step and "eval_NDCG@10" in log:
                    best_log_entry = log
                    break
            
            if best_log_entry:
                print(f"\nðŸ“Š Detailed Metrics for Best Model (Step {best_step}):")
                # æ ¼å¼åŒ–æ‰“å°å­—å…¸
                for k, v in best_log_entry.items():
                    if k.startswith("eval_"):
                        print(f"   - {k}: {v}")
            else:
                print(f"âš ï¸ Could not find detailed logs for step {best_step} in history.")

        except Exception as e:
            print(f"âš ï¸ Error parsing best step info: {e}")

    # ================= ä¿å­˜æœ€ç»ˆæ¨¡åž‹ =================
    final_save_path = os.path.join(output_dir, "final_best_grpo_model")
    trainer.save_model(final_save_path)
    print(f">>> Training Finished & Best Model Saved to {final_save_path}")