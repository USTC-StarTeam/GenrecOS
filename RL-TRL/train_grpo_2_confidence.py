# train_grpo.py
import sys
import os
import yaml
import argparse
from datetime import datetime
import torch
from datasets import load_dataset
from tqdm import tqdm
from trl import GRPOConfig
from transformers import (
    AutoConfig, 
    AutoModelForCausalLM, 
    AutoTokenizer, 
    LogitsProcessorList, 
    TrainerCallback, 
    EarlyStoppingCallback,
    LlamaForCausalLM,
    Qwen2ForCausalLM,
)
from trl.trainer.utils import (
    nanmax,
    nanmin,
)
from torch.utils.data import DataLoader
import numpy as np
import random

# ================= é…ç½®è·¯å¾„ =================
# 1. ç¡®ä¿èƒ½å¯¼å…¥ç›¸å…³æ¨¡å—
sys.path.append("../Rec-Transformer") # LlamaRec æ‰€åœ¨
sys.path.append("../")


# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from CTR_models.src.DIN_evaluator import DINScorer
from llamarec import LlamaRecConfig, LlamaRecForCausalLM 
from sasrec import SasRecForCausalLM
from utils.RL_utils import GRPOTrainer_not_skip_special_token, RewardRunner, RewardRunner_wo_gt
from utils.datacollator import EvalDataCollator, preprocess_function
from utils.utils_evaluate import DynamicHierarchicalLogitsProcessor, build_item_token_codebooks_dynamically
from utils.eval import compute_hr_at_k, compute_ndcg_at_k
from utils.utils import *

# ================= æ³¨å†Œæ¨¡å‹ =================
AutoConfig.register("llama-rec", LlamaRecConfig)
AutoModelForCausalLM.register(LlamaRecConfig, LlamaRecForCausalLM)

class GRPO_Eval_Trainer(GRPOTrainer_not_skip_special_token):
    def __init__(self, eval_dataset, generation_config_params, eval_collator, **kwargs):
        """
        ç»§æ‰¿è‡ª GRPOTrainerï¼Œä½†æ³¨å…¥äº†ä¸ Script B å®Œå…¨å¯¹é½çš„è¯„æµ‹é€»è¾‘
        """
        self.custom_eval_dataset = eval_dataset
        self.eval_collator = eval_collator  # æ¥æ”¶æ¥è‡ª Script B çš„ Collator
        
        # è§£åŒ…ç”Ÿæˆå‚æ•°
        self.gen_len = generation_config_params.get('generation_length', 4)
        self.num_beams = generation_config_params.get('num_beams', 1)
        self.k_values = generation_config_params.get('k_values', [1, 5, 10])
        self.item_token_codebooks = generation_config_params.get('item_token_codebooks', None)
        # self.token_pos_weights = torch.tensor([1.0, 0.5, 0.3, 0.2], dtype=torch.float32)
        self.eval_sample_num = generation_config_params.get('eval_sample_num', 2000)

        # --- æ„å»º NumPy å‘é‡åŒ–æŸ¥æ‰¾è¡¨ ---
        print(">>> Building NumPy Vectorized Vocab Table for Evaluation...")
        vocab = kwargs['processing_class'].get_vocab()
        max_id = max(vocab.values())
        self.vocab_array = np.array(["" for _ in range(max_id + 1)], dtype=object)
        for k, v in vocab.items():
            self.vocab_array[v] = k
        print(">>> âœ… Vocab Table built.")

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        # æ³¨æ„ï¼šçˆ¶ç±»ä¸éœ€è¦ eval_datasetï¼Œå› ä¸ºæˆ‘ä»¬è¦åœ¨ evaluate ä¸­æ‰‹åŠ¨å¤„ç†å®ƒ
        # é¿å…çˆ¶ç±»å¯¹æˆ‘ä»¬çš„ eval_dataset åšä¸å¿…è¦çš„åˆ—ç§»é™¤æ“ä½œ
        super().__init__(eval_dataset=eval_dataset, **kwargs)

    # é‡å†™ evaluate æ–¹æ³•
    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ datasetï¼Œå¦åˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶é¢„å¤„ç†å¥½çš„ dataset
        eval_ds = eval_dataset if eval_dataset is not None else self.custom_eval_dataset
        
        if eval_ds is None:
            print(">>> Warning: No eval dataset provided, skipping evaluation.")
            return {}
        
        if metric_key_prefix == "eval" and eval_ds is not None:
            total_size = len(eval_ds)
            if total_size > self.eval_sample_num:
                print(f"âš¡ [SpeedUp] Sampling {self.eval_sample_num} random examples from {total_size} for validation.")
                
                # éšæœºé€‰å–ç´¢å¼•
                # æ³¨æ„ï¼šè¿™é‡Œæ¯æ¬¡éªŒè¯éƒ½ä¼šé‡æ–°éšæœºï¼Œå¯¼è‡´éªŒè¯æŒ‡æ ‡ä¼šæœ‰æ³¢åŠ¨ï¼Œä½†èƒ½æ›´å…¨é¢åœ°ç›‘æ§æ¨¡å‹
                random_indices = random.sample(range(total_size), self.eval_sample_num)
                
                # ä½¿ç”¨ HuggingFace dataset çš„ select æ–¹æ³•åˆ›å»ºå­é›†
                eval_ds = eval_ds.select(random_indices)
            else:
                print(f"Dataset size ({total_size}) <= {self.eval_sample_num}, running full evaluation.")

        # 1. å‡†å¤‡ DataLoader
        # ç¡®ä¿æ•°æ®ç»è¿‡äº† preprocess_function å¤„ç†
        batch_size = self.args.per_device_eval_batch_size or self.args.per_device_train_batch_size
        eval_dataloader = DataLoader(
            eval_ds,
            batch_size=batch_size,
            collate_fn=self.eval_collator, # ä½¿ç”¨ä¼ å…¥çš„æ­£ç¡® Collator
            shuffle=False,
            drop_last=False
        )

        # åˆ‡æ¢æ¨¡å¼
        model = self.model
        model.eval()
        
        print(f"\n***** Running Generative Evaluation (Step {self.state.global_step}) *****")
        print(f"  Num examples = {len(eval_ds)}")
        print(f"  Batch size = {batch_size}")
        
        total_metrics_sum = {f"HR@{k}": 0.0 for k in self.k_values}
        total_metrics_sum.update({f"NDCG@{k}": 0.0 for k in self.k_values})
        total_samples = 0

        # 2. å¾ªç¯ç”Ÿæˆ
        with torch.no_grad():
            for batch in tqdm(eval_dataloader, desc="Evaluating"):
                input_ids = batch['input_ids'].to(self.args.device)
                attention_mask = batch['attention_mask'].to(self.args.device)
                groundtruth = batch['groundtruth'] # List[str]

                curr_bs = input_ids.shape[0]
                prompt_length = input_ids.shape[1]

                # æ„é€  Logits Processor (å¦‚æœæä¾›äº† codebooks)
                logits_processor = LogitsProcessorList()
                if self.item_token_codebooks:
                    logits_processor.append(
                        DynamicHierarchicalLogitsProcessor(
                            prompt_length=prompt_length,
                            item_token_codebooks=self.item_token_codebooks,
                            device=self.args.device
                        )
                    )

                # 3. Beam Search ç”Ÿæˆ (HuggingFace Generate)
                generated_ids = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=prompt_length + self.gen_len,
                    num_beams=self.num_beams,
                    do_sample=False, 
                    num_return_sequences=self.num_beams,
                    pad_token_id=self.processing_class.pad_token_id,
                    eos_token_id=self.processing_class.eos_token_id,
                    logits_processor=logits_processor,
                    use_cache=True
                )

                # 4. ã€æé€Ÿè§£ç ã€‘(NumPy Vectorized)
                # shape: [Batch_Size * Num_Beams, Gen_Len]
                new_tokens_cpu = generated_ids[:, -self.gen_len:].cpu().numpy()
                token_strs = self.vocab_array[new_tokens_cpu] # O(1) æŸ¥è¡¨
                
                # å‘é‡åŒ–å­—ç¬¦ä¸²æ‹¼æ¥
                if self.gen_len == 1:
                    predicted_token_sequences = token_strs.flatten().tolist()
                else:
                    result_array = token_strs[:, 0]
                    for i in range(1, self.gen_len):
                        result_array = result_array + token_strs[:, i]
                    predicted_token_sequences = result_array.tolist()

                # 5. Reshape ä¸º [Batch, Num_Beams]
                reshaped_token_sequences = [
                    predicted_token_sequences[i : i + self.num_beams]
                    for i in range(0, len(predicted_token_sequences), self.num_beams)
                ]

                # 6. è®¡ç®—æŒ‡æ ‡
                batch_hr = compute_hr_at_k(reshaped_token_sequences, groundtruth, self.k_values)
                batch_ndcg = compute_ndcg_at_k(reshaped_token_sequences, groundtruth, self.k_values)

                for k_val in self.k_values:
                    total_metrics_sum[f"HR@{k_val}"] += batch_hr[f"HR@{k_val}"] * curr_bs
                    total_metrics_sum[f"NDCG@{k_val}"] += batch_ndcg[f"NDCG@{k_val}"] * curr_bs
                
                total_samples += curr_bs
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        model.train()

        # 7. æ±‡æ€»å¹¶ Log
        metrics = {f"{metric_key_prefix}_{k}": (v / total_samples) for k, v in total_metrics_sum.items()}
        
        # è°ƒç”¨ Trainer å†…ç½®çš„ log æ–¹æ³•ï¼Œè¿™æ · Wandb å’Œæ—¥å¿—æ–‡ä»¶éƒ½èƒ½è®°å½•åˆ°
        self.log(metrics)
        
        # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼šå°† metric ä¼ å›ç»™ Trainer çš„ control ç³»ç»Ÿï¼Œç”¨äºæ—©åœåˆ¤æ–­
        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)
        
        # ä»æ‰“å°ç»“æœæ¥çœ‹ï¼Œå¥½åƒtransformersä¼šè‡ªåŠ¨æ‰“å°metricï¼Œå’Œä¸‹é¢çš„å‡ ä¹åˆ«æ— äºŒè‡´
        # print(f">>> Evaluation Metrics: {metrics}")
        return metrics

class GRPO_Eval_Trainer_Confidence_Aware(GRPO_Eval_Trainer):
   # ä¸»è¦ä¼ å…¥token_pos_weights
    def _compute_loss(self, model, inputs):
        # Compute the per-token log probabilities for the model
        prompt_ids, prompt_mask = inputs["prompt_ids"], inputs["prompt_mask"]
        completion_ids, completion_mask = inputs["completion_ids"], inputs["completion_mask"]
        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
        logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens

        # Compute the per_token_logps and the entropy at each position in the completion
        per_token_logps, entropies = self._get_per_token_logps_and_entropies(
            model,
            input_ids,
            attention_mask,
            logits_to_keep,
            compute_entropy=True,
            pixel_values=inputs.get("pixel_values"),
            image_grid_thw=inputs.get("image_grid_thw"),
            num_images=inputs.get("num_images"),
            pixel_attention_mask=inputs.get("pixel_attention_mask"),
            image_sizes=inputs.get("image_sizes"),
            token_type_ids=inputs.get("token_type_ids"),
        )

        if self.top_entropy_quantile < 1.0:
            mask = completion_mask if not self.tools else completion_mask * inputs["tool_mask"]
            entropy_mask = self.get_high_entropy_mask(entropies, mask, 1 - self.top_entropy_quantile)
        else:
            entropy_mask = None

        # Compute the loss
        advantages = inputs["advantages"]
        # In the base GRPO implementation, advantages are expected to have shape (B,). To support subclasses that
        # provide advantages with shape (B, T) (e.g., MiniLLM), we *conditionally* unsqueeze the tensor.
        if advantages.dim() == 1:
            advantages = advantages.unsqueeze(1)
        # When num_iterations == 1 and steps_per_generation <= gradient_accumulation_steps,
        # old_per_token_logps == per_token_logps. In this case we can skip its computation
        # (see _generate_and_score_completions) and instead use per_token_logps.detach().
        # The exception is when using vLLM, where we always compute old_per_token_logps
        # for importance sampling
        old_per_token_logps = inputs.get("old_per_token_logps")
        old_per_token_logps = per_token_logps.detach() if old_per_token_logps is None else old_per_token_logps

        log_ratio = per_token_logps - old_per_token_logps
        if self.importance_sampling_level == "token":
            log_importance_weights = log_ratio
        elif self.importance_sampling_level == "sequence":
            mask = completion_mask if not self.tools else completion_mask * inputs["tool_mask"]
            log_importance_weights = (log_ratio * mask).sum(-1) / mask.sum(-1).clamp(min=1.0)
            log_importance_weights = log_importance_weights.unsqueeze(-1)
        else:
            raise ValueError(
                f"Unknown importance sampling level: {self.importance_sampling_level}. Possible values are 'token' "
                "and 'sequence'."
            )

        coef_1 = torch.exp(log_importance_weights)

        # Compute the KL divergence between the model and the reference model
        if self.beta != 0.0:
            ref_per_token_logps = inputs["ref_per_token_logps"]
            per_token_kl = (
                torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
            )
            # Importance sampling correction for the KL divergence
            if self.args.use_bias_correction_kl:
                per_token_kl = per_token_kl * coef_1

        # From here, log_importance_weights (and all subsequent tensors, coef_1, coef_2, etc.) shape depends on
        # importance_sampling_level: "token" level: (B, T); "sequence" level: (B, 1)
        if self.loss_type == "cispo":
            clamped_ratios = torch.clamp(coef_1, max=self.epsilon_high).detach()
            per_token_loss = -clamped_ratios * advantages * per_token_logps
        elif self.loss_type in ["grpo", "bnpo", "dr_grpo", "dapo"]:
            coef_2 = torch.clamp(coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)
            # Two-sided clipping
            if self.args.delta is not None:
                coef_1 = torch.clamp(coef_1, max=self.args.delta)

            per_token_loss1 = coef_1 * advantages
            per_token_loss2 = coef_2 * advantages
            per_token_loss = -torch.min(per_token_loss1, per_token_loss2)
        elif self.loss_type == "sapo":
            per_token_loss = torch.empty_like(coef_1)
            positive_advantages_mask = advantages.repeat([1, coef_1.shape[1]]) > 0
            per_token_loss[positive_advantages_mask] = self.get_sapo_token_loss(
                coef_1[positive_advantages_mask], self.args.sapo_temperature_pos
            )
            per_token_loss[~positive_advantages_mask] = self.get_sapo_token_loss(
                coef_1[~positive_advantages_mask], self.args.sapo_temperature_neg
            )
            per_token_loss = -per_token_loss * advantages
        else:
            raise ValueError(f"Unknown loss type: {self.loss_type}")

        if entropy_mask is not None:
            per_token_loss = per_token_loss * entropy_mask

        if self.use_vllm and self.vllm_importance_sampling_correction:
            per_token_loss = per_token_loss * inputs["importance_sampling_ratio"]

        if self.beta != 0.0:
            per_token_loss = per_token_loss + self.beta * per_token_kl

        mask = completion_mask if not self.tools else completion_mask * inputs["tool_mask"]

        # åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºè®¡ç®— Loss çš„ maskï¼Œé¿å…æ±¡æŸ“ç”¨äºè®°å½•æ—¥å¿—çš„åŸå§‹ mask
        loss_mask = mask.clone().float()

        # ================== ğŸ”µ æ–°å¢ï¼šåŠ¨æ€ç½®ä¿¡åº¦åŠ æƒ (Confidence-Aware Weighting) ğŸ”µ ==================
        # ç›®çš„ï¼šå¯¹äº Advantage > 0 (å¥½æ ·æœ¬)ï¼Œå…³æ³¨ç½®ä¿¡åº¦ä½çš„ (éš¾æ ·æœ¬)
        #       å¯¹äº Advantage < 0 (åæ ·æœ¬)ï¼Œå…³æ³¨ç½®ä¿¡åº¦é«˜çš„ (å‚²æ…¢æ ·æœ¬)
        
        # 1. è·å–å½“å‰ Token çš„ç”Ÿæˆæ¦‚ç‡ P (åˆ‡æ–­æ¢¯åº¦ï¼Œåªä½œä¸ºæƒé‡ç³»æ•°)
        token_probs = torch.exp(per_token_logps.detach())
        
        # 2. æ‰©å±• Advantage ç»´åº¦ä»¥åŒ¹é… Token åºåˆ— (B, 1) -> (B, T)
        # æ³¨æ„ï¼šinputs["advantages"] é€šå¸¸æ˜¯ (B) æˆ– (B, 1)
        advantages_broad = inputs["advantages"]
        if advantages_broad.dim() == 1:
            advantages_broad = advantages_broad.unsqueeze(1)
        
        # 3. å®šä¹‰æ•æ„Ÿåº¦ç³»æ•° lambda (å»ºè®® 0.5 ~ 1.0ï¼Œå¤ªå¤§ä¼šå¯¼è‡´æ¢¯åº¦æ–¹å·®è¿‡å¤§)
        conf_sensitivity = 2.0 
        
        # 4. æ ¹æ® Advantage æ­£è´Ÿæ„å»ºåŠ¨æ€æƒé‡
        # logic: positive_adv -> weight += (1 - p)
        #        negative_adv -> weight += p
        # ä½¿ç”¨ torch.where å®ç°æ¡ä»¶é€‰æ‹©
        
        # åˆ¤æ–­å¥½åç»“æœ (å¹¿æ’­åˆ°åºåˆ—ç»´åº¦)
        is_positive = (advantages_broad > 0).expand_as(token_probs)
        
        # è®¡ç®—åŸºç¡€åŠ¨æ€é¡¹
        dynamic_term = torch.where(
            is_positive,
            1.0 - token_probs,  # å¥½ç»“æœï¼šæ¦‚ç‡è¶Šä½(è¶Šä¸ç¡®å®š)ï¼Œé¡¹è¶Šå¤§
            token_probs         # åç»“æœï¼šæ¦‚ç‡è¶Šé«˜(è¶Šè‡ªä¿¡)ï¼Œé¡¹è¶Šå¤§
        )
        
        # ç”Ÿæˆæœ€ç»ˆçš„ç½®ä¿¡åº¦æƒé‡çŸ©é˜µ (Base 1.0 + åŠ¨æ€é¡¹)
        conf_weights = 1.0 + (conf_sensitivity * dynamic_term)
        
        # 5. å°†åŠ¨æ€æƒé‡åº”ç”¨åˆ° loss_mask ä¸Š
        loss_mask = loss_mask * conf_weights
        # =======================================================================================

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ token_pos_weights (åœ¨ __init__ ä¸­å®šä¹‰çš„)
        if hasattr(self, "token_pos_weights") and self.token_pos_weights is not None:
            # 1. è½¬æ¢è®¾å¤‡å’Œç²¾åº¦ä»¥åŒ¹é… loss
            pos_weights = self.token_pos_weights.to(device=per_token_loss.device, dtype=per_token_loss.dtype)
            
            # 2. å°†æƒé‡åº”ç”¨åˆ° loss_mask çš„æœ«å°¾
            # å‡è®¾ç”Ÿæˆé•¿åº¦å›ºå®šä¸º 4ï¼Œæˆ‘ä»¬å¯¹é½åºåˆ—çš„æœ€å gen_len ä½
            gen_len = len(pos_weights)
            seq_len = loss_mask.shape[1]
            
            if seq_len >= gen_len:
                # å¹¿æ’­ä¹˜æ³•ï¼š(Batch, gen_len) *= (gen_len,)
                loss_mask[:, -gen_len:] *= pos_weights
            else:
                # å®¹é”™å¤„ç†ï¼šå¦‚æœå®é™…åºåˆ—æ¯”æƒé‡çŸ­ï¼Œæˆªå–æƒé‡çš„ååŠéƒ¨åˆ†
                loss_mask *= pos_weights[-seq_len:]

        # â• [Modified] Use 'loss_mask' instead of 'mask' for loss calculation
        # æ³¨æ„ï¼šåˆ†æ¯ä¹Ÿè¦å˜æˆ weighted sumï¼Œè¿™æ ·æ‰æ˜¯åŠ æƒå¹³å‡
        loss1 = ((per_token_loss * loss_mask).sum(-1) / loss_mask.sum(-1).clamp(min=1.0)).mean()
        loss1 = loss1 / self.current_gradient_accumulation_steps
        
        if self.loss_type in ["grpo", "sapo"]:
            loss = ((per_token_loss * loss_mask).sum(-1) / loss_mask.sum(-1).clamp(min=1.0)).mean()
            loss = loss / self.current_gradient_accumulation_steps
        elif self.loss_type == "bnpo":
            loss = (per_token_loss * loss_mask).sum() / loss_mask.sum().clamp(min=1.0)
            loss = loss / self.current_gradient_accumulation_steps
        elif self.loss_type == "dr_grpo":
            # DR_GRPO é€šå¸¸åˆ†æ¯æ˜¯å›ºå®šé•¿åº¦ï¼Œè¿™é‡Œæš‚æ—¶ä¿æŒ maskï¼Œæˆ–è€…ä½ ä¹Ÿæƒ³åŠ æƒï¼Ÿ
            # å»ºè®® DR_GRPO ä¹Ÿç”¨åŠ æƒåçš„ loss_maskï¼Œä¿æŒé€»è¾‘ä¸€è‡´
            loss = (per_token_loss * loss_mask).sum() / (per_token_loss.size(0) * self.max_completion_length)
            loss = loss / self.current_gradient_accumulation_steps
        elif self.loss_type in ["cispo", "dapo"]:
            normalizer = inputs["num_items_in_batch"] / self.accelerator.num_processes
            loss = (per_token_loss * loss_mask).sum() / normalizer
        else:
            raise ValueError(f"Unknown loss type: {self.loss_type}")

        # Log the metrics
        mode = "train" if self.model.training else "eval"

        completion_token_count = mask.sum().clamp(min=1.0)

        def masked_batch_mean(x):
            if x.shape[1] == 1:  # when importance_sampling_level == "sequence"
                return x.mean()
            else:
                return (x * mask).sum() / completion_token_count

        if self.beta != 0.0:
            mean_kl = masked_batch_mean(per_token_kl)
            self._metrics[mode]["kl"].append(self.accelerator.gather(mean_kl).nanmean().item())

        mean_entropy = masked_batch_mean(entropies)
        self._metrics[mode]["entropy"].append(self.accelerator.gather(mean_entropy).nanmean().item())

        if self.loss_type in ["grpo", "bnpo", "dr_grpo", "dapo"]:
            # Compute the clipped probability ratios
            is_low_clipped = (coef_1 < 1 - self.epsilon_low) & (advantages < 0)
            is_high_clipped = (coef_1 > 1 + self.epsilon_high) & (advantages > 0)
            is_region_clipped = is_low_clipped | is_high_clipped

            low_clip = masked_batch_mean(is_low_clipped.float())
            high_clip = masked_batch_mean(is_high_clipped.float())
            clip_ratio = masked_batch_mean(is_region_clipped.float())

            gathered_low_clip = self.accelerator.gather(low_clip)
            self._metrics[mode]["clip_ratio/low_mean"].append(gathered_low_clip.nanmean().item())
            self._metrics[mode]["clip_ratio/low_min"].append(nanmin(gathered_low_clip).item())
            gathered_high_clip = self.accelerator.gather(high_clip)
            self._metrics[mode]["clip_ratio/high_mean"].append(gathered_high_clip.nanmean().item())
            self._metrics[mode]["clip_ratio/high_max"].append(nanmax(gathered_high_clip).item())
            gathered_clip_ratio = self.accelerator.gather(clip_ratio)
            self._metrics[mode]["clip_ratio/region_mean"].append(gathered_clip_ratio.nanmean().item())
        elif self.loss_type == "cispo":
            is_cispo_clipped = (coef_1 > self.epsilon_high) & (advantages > 0)
            cispo_clip_ratio = masked_batch_mean(is_cispo_clipped.float())
            gathered_cispo_clip_ratio = self.accelerator.gather(cispo_clip_ratio)
            self._metrics[mode]["cispo_clip_ratio"].append(gathered_cispo_clip_ratio.nanmean().item())
        return loss

if __name__ == '__main__':
    # ================= å‘½ä»¤è¡Œå‚æ•°è§£æ =================
    parser = argparse.ArgumentParser(description="GRPO Training with YAML Config")
    parser.add_argument("--config", type=str, default="./rl_configs/KuaiRec_big_llamarec_DIN.yaml", help="Path to the YAML config file")
    args_cli = parser.parse_args()

    # ================= åŠ è½½ config =================
    print(f">>> Loading configuration from {args_cli.config}...")
    with open(args_cli.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # æå–å„ä¸ªéƒ¨åˆ†çš„é…ç½®
    paths_cfg = config['paths']
    din_cfg = config['din']
    train_cfg = config['training']
    eval_cfg = config['evaluation']

    # ================= åŠ è½½ DIN Scorer =================
    print(">>> Initializing DIN Reward Model...")
    
    din_scorer = DINScorer(
        config_dir=paths_cfg['din_config_dir'],
        model_dir=paths_cfg['din_model_dir'],
        experiment_id=din_cfg['experiment_id'],
        data_dir=paths_cfg['din_data_dir'],
        device=din_cfg['device']                   
    )
    print(">>> DIN Scorer Ready.")

    reward_runner = RewardRunner(
        scorer=din_scorer, 
        weight=din_cfg['reward_weight'],
        penalty=din_cfg.get('penalty', -1.0)
    )

    # ================= åŠ è½½æ¨¡å‹ä¸Tokenizer =================
    print(">>> Loading LLM...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    llm_path = paths_cfg['llm_model_path']
    # åŠ¨æ€é€‰æ‹©æ¨¡å‹ç±»
    if train_cfg['model_name'] == 'sasrec':
        model_class = SasRecForCausalLM
    elif train_cfg['model_name'] == 'llama':
        model_class = LlamaForCausalLM
    elif train_cfg['model_name'].startswith('qwen'):
        model_class = Qwen2ForCausalLM
    else:
        model_class = LlamaRecForCausalLM

    try:
        model = model_class.from_pretrained(
            llm_path, 
            torch_dtype=torch.bfloat16 if train_cfg.get('bf16', False) else "auto",
            device_map=device
        )
    except Exception as e:
        print(f"Failed to load model from {llm_path}. Error: {e}")
        # å°è¯•è‡ªåŠ¨å›é€€
        print("Trying AutoModelForCausalLM...")
        model = AutoModelForCausalLM.from_pretrained(llm_path, device_map=device)
    tokenizer = AutoTokenizer.from_pretrained(llm_path, trust_remote_code=True)

    # Tokenizer è¡¥ä¸
    if tokenizer.model_input_names is not None and "token_type_ids" in tokenizer.model_input_names:
        tokenizer.model_input_names.remove("token_type_ids")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'
    tokenizer.truncation_side = 'left'

    # ================= å‡†å¤‡è¯„ä¼°ç”¨çš„ Codebook =================
    generation_length = eval_cfg['generation_length']
    item_token_codebooks = build_item_token_codebooks_dynamically(tokenizer, generation_length)

    # ================= æ•°æ®é›†åŠ è½½ =================
    rl_data_dir = paths_cfg['rl_data_dir']
    
    # 1. åŠ è½½ Train
    train_json_path = os.path.join(rl_data_dir, paths_cfg['train_file'])
    print(f">>> Loading Train Dataset: {train_json_path}")
    train_dataset = load_dataset("json", data_files=train_json_path, split="train")

    # 2. åŠ è½½ Test
    test_json_path = os.path.join(rl_data_dir, paths_cfg['test_file'])
    print(f">>> Loading Test Dataset: {test_json_path}")
    raw_test_dataset = load_dataset("json", data_files=test_json_path, split="train")

    # ================= è®­ç»ƒè¾“å‡ºè·¯å¾„é…ç½® =================
    base_dir = paths_cfg['output_root']
    date_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(base_dir, f"checkpoints_{date_time}")
    os.makedirs(output_dir, exist_ok=True)
    print(f">>> Checkpoints will be saved to: {output_dir}")

    max_seq_length = train_cfg['max_seq_length']
    print(f">>> Preprocessing Test Dataset... Max Length: {max_seq_length}")
    # ä½¿ç”¨ Script B åŒæ¬¾çš„ preprocess_function
    test_dataset = raw_test_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=8, 
        remove_columns=['prompt'], # è¿™é‡Œè¦æ³¨æ„ï¼špreprocess_function ä¼šç”Ÿæˆ input_idsï¼Œæˆ‘ä»¬åªä¿ç•™éœ€è¦çš„åˆ—
        fn_kwargs={"tokenizer": tokenizer, "max_seq_length": max_seq_length},
        desc="Tokenizing Test Set"
    )
    eval_collator = EvalDataCollator(tokenizer=tokenizer, max_length=max_seq_length)

    # é€»è¾‘ï¼šæ€»æ—¥å¿—ç›®å½• / æœ¬æ¬¡å®éªŒåç§°(å¸¦æ—¶é—´æˆ³)
    # è¿™æ ·ä½ åœ¨ tensorboard --logdir ./temp_try_GRPO_Rec_Output/all_tensorboard_logs æ—¶èƒ½çœ‹åˆ°æ‰€æœ‰å®éªŒçš„æ›²çº¿å¯¹æ¯”
    tb_root = paths_cfg.get('tensorboard_root', './temp_try_GRPO_Rec_Output/all_tensorboard_logs')
    tb_dir = os.path.join(tb_root, f"run_{date_time}")
    
    print(f">>> TensorBoard logs will be saved to: {tb_dir}")

    # ================= è®­ç»ƒå‚æ•°é…ç½® =================
    # ä» yaml ä¸­æå– evaluate/save çš„æ­¥æ•°
    eval_save_steps = train_cfg['eval_save_steps']

    training_args = GRPOConfig(
        output_dir=output_dir,
        logging_dir = tb_dir,
        report_to="tensorboard",
        
        learning_rate=float(train_cfg['learning_rate']), # ç¡®ä¿ YAML è¯»å–çš„æ˜¯ float
        num_train_epochs=train_cfg['num_train_epochs'],
        per_device_train_batch_size=train_cfg['per_device_train_batch_size'],
        gradient_accumulation_steps=train_cfg['gradient_accumulation_steps'],
        
        # æ—¥å¿—
        logging_steps=train_cfg['logging_steps'],
        
        # ç”Ÿæˆå‚æ•° (RL Training)
        max_completion_length=train_cfg['max_completion_length'],
        num_generations=train_cfg['num_generations'],
        use_vllm=train_cfg['use_vllm'],
        bf16=train_cfg['bf16'] if 'bf16' in train_cfg else False,
        mask_truncated_completions=train_cfg.get('mask_truncated_completions', False),
        temperature=0.7,        # ç¨å¾®é™ä¸€ç‚¹ï¼Œé¿å…ç”Ÿæˆå®Œå…¨ä¹±ç çš„ Item ID
        top_k=50,               # é™åˆ¶é‡‡æ ·èŒƒå›´ï¼Œé¿å…é‡‡æ ·åˆ°æå…¶å†·é—¨çš„ Item
        top_p=0.95,

        # è¯„ä¼°ä¸æ—©åœç­–ç•¥
        eval_strategy="steps",
        eval_steps=eval_save_steps,
        per_device_eval_batch_size=train_cfg['per_device_eval_batch_size'],

        save_strategy="steps",
        save_steps=eval_save_steps,
        save_total_limit=train_cfg['save_total_limit'],
        
        load_best_model_at_end=True,
        metric_for_best_model=train_cfg['metric_for_best_model'],
        greater_is_better=True,
        
        # é˜²æ­¢åˆ æ‰ prompt/ground_truth åˆ—
        remove_unused_columns=False
    )
    
    # ç»„è£…è¯„ä¼°é…ç½®å­—å…¸
    eval_config_dict = {
        "generation_length": generation_length,
        "num_beams": eval_cfg['num_beams'],
        "k_values": eval_cfg['k_values'],
        "item_token_codebooks": item_token_codebooks,
        "eval_sample_num": eval_cfg.get('eval_sample_num', 2000)
    }

    # ================= åˆå§‹åŒ– Trainer =================
    trainer = GRPO_Eval_Trainer_Confidence_Aware(
        model=model,
        reward_funcs=[reward_runner],
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        eval_collator=eval_collator,
        generation_config_params=eval_config_dict,
        processing_class=tokenizer,
        
        # æ—©åœå›è°ƒ
        callbacks=[EarlyStoppingCallback(
            early_stopping_patience=train_cfg['early_stopping_patience']
        )]
    )

    # ================= å¼€å§‹è®­ç»ƒ =================
    print(">>> Starting GRPO Training with Live Evaluation...")
    trainer.train()

    # ================= æ‰“å°æœ€ä½³æ¨¡å‹ç»“æœ =================
    # è·å–æœ€ä½³ Checkpoint çš„è·¯å¾„
    best_ckpt_path = trainer.state.best_model_checkpoint
    
    if best_ckpt_path:
        print(f"\n" + "="*50)
        print(f"ğŸ† TRAINING FINISHED. BEST MODEL FOUND.")
        print(f"="*50)
        print(f"ğŸ“ Best Checkpoint Path: {best_ckpt_path}")
        print(f"ğŸŒŸ Best Metric Value:    {trainer.state.best_metric}")
        
        # --- æ ¸å¿ƒé€»è¾‘ï¼šä»æ—¥å¿—å†å²ä¸­æå‡ºæœ€ä½³é‚£ä¸€æ­¥çš„å®Œæ•´æŒ‡æ ‡ ---
        # 1. ä»è·¯å¾„ä¸­æå–æœ€ä½³æ­¥æ•° (ä¾‹å¦‚ "xxx/checkpoint-500" -> 500)
        try:
            best_step = int(best_ckpt_path.split('-')[-1])
            
            # 2. éå†æ—¥å¿—å†å²æ‰¾åˆ°é‚£ä¸€åˆ»çš„è¯¦ç»†æ•°æ®
            best_log_entry = None
            for log in trainer.state.log_history:
                # å¿…é¡»åŒæ—¶æ»¡è¶³ï¼šæ˜¯è¿™ä¸€æ­¥ï¼Œä¸”åŒ…å«è¯„ä¼°æŒ‡æ ‡(æ¯”å¦‚æœ‰ eval_loss æˆ– eval_NDCG@10)
                if log.get("step") == best_step and "eval_NDCG@10" in log:
                    best_log_entry = log
                    break
            
            if best_log_entry:
                print(f"\nğŸ“Š Detailed Metrics for Best Model (Step {best_step}):")
                # æ ¼å¼åŒ–æ‰“å°å­—å…¸
                for k, v in best_log_entry.items():
                    if k.startswith("eval_"):
                        print(f"   - {k}: {v}")
            else:
                print(f"âš ï¸ Could not find detailed logs for step {best_step} in history.")

        except Exception as e:
            print(f"âš ï¸ Error parsing best step info: {e}")

    # ================= ä¿å­˜æœ€ç»ˆæ¨¡å‹ =================
    final_save_path = os.path.join(output_dir, "final_best_grpo_model")
    trainer.save_model(final_save_path)
    print(f">>> Training Finished & Best Model Saved to {final_save_path}")